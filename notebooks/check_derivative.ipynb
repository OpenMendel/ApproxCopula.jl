{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff to check gradient/Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling QuasiCopula [c47b6ae2-b804-4668-9957-eb588c99ffbc]\n",
      "└ @ Base loading.jl:1423\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mCompiling VCF parser...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_make_snparray (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using DataFrames, Random, GLM, QuasiCopula\n",
    "using ForwardDiff, Test, LinearAlgebra\n",
    "using LinearAlgebra: BlasReal, copytri!\n",
    "using ToeplitzMatrices\n",
    "using BenchmarkTools\n",
    "using SnpArrays\n",
    "using ForwardDiff\n",
    "# using MendelPlots\n",
    "ENV[\"COLUMNS\"] = 240\n",
    "\n",
    "\n",
    "BLAS.set_num_threads(1)\n",
    "Threads.nthreads()\n",
    "\n",
    "function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "    p::Int64; mafs::Vector{Float64}=zeros(Float64, p), min_ma::Int = 5)\n",
    "\n",
    "    #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "    A1 = BitArray(undef, n, p) \n",
    "    A2 = BitArray(undef, n, p) \n",
    "    for j in 1:p\n",
    "        minor_alleles = 0\n",
    "        maf = 0\n",
    "        while minor_alleles <= min_ma\n",
    "            maf = 0.5rand()\n",
    "            for i in 1:n\n",
    "                A1[i, j] = rand(Bernoulli(maf))\n",
    "                A2[i, j] = rand(Bernoulli(maf))\n",
    "            end\n",
    "            minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "        end\n",
    "        mafs[j] = maf\n",
    "    end\n",
    "\n",
    "    #fill the SnpArray with the corresponding x_tmp entry\n",
    "    return _make_snparray(s, A1, A2)\n",
    "end\n",
    "\n",
    "function _make_snparray(s::Union{String, UndefInitializer}, A1::BitArray, A2::BitArray)\n",
    "    n, p = size(A1)\n",
    "    x = SnpArray(s, n, p)\n",
    "    for i in 1:(n*p)\n",
    "        c = A1[i] + A2[i]\n",
    "        if c == 0\n",
    "            x[i] = 0x00\n",
    "        elseif c == 1\n",
    "            x[i] = 0x02\n",
    "        elseif c == 2\n",
    "            x[i] = 0x03\n",
    "        else\n",
    "            throw(MissingException(\"matrix shouldn't have missing values!\"))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qc_model = Quasi-Copula Variance Component Model\n",
      "  * base distribution: Bernoulli\n",
      "  * link function: LogitLink\n",
      "  * number of clusters: 5000\n",
      "  * cluster size min, max: 5, 5\n",
      "  * number of variance components: 2\n",
      "  * number of fixed effects: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function simulate_VC_longitudinal(;\n",
    "    n = 1000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of nongenetic covariates, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = 10, # number of causal SNPs\n",
    "    seed = 2022,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    )\n",
    "    m == 1 || m == 2 || error(\"m (number of VC) must be 1 or 2\")\n",
    "    \n",
    "    # non-genetic effect sizes\n",
    "    Random.seed!(seed)\n",
    "    βtrue = [1.0; rand(-0.03:0.06:0.03, p-1)]\n",
    "    dist = y_distribution()\n",
    "    link = canonicallink(dist)\n",
    "    Dist = typeof(dist)\n",
    "    Link = typeof(link)\n",
    "\n",
    "    # variance components\n",
    "    θtrue = fill(0.1, m)\n",
    "    V1 = ones(d, d)\n",
    "    V2 = Matrix(I, d, d)\n",
    "    Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "\n",
    "    # simulate design matrices\n",
    "    Random.seed!(seed)\n",
    "    X_full = [hcat(ones(d), randn(d, p - 1)) for i in 1:n]\n",
    "\n",
    "    # simulate random SnpArray with 100 SNPs and randomly choose k SNPs to be causal\n",
    "    Random.seed!(2022)\n",
    "    G = simulate_random_snparray(undef, n, q)\n",
    "    Gfloat = convert(Matrix{T}, G, center=true, scale=false)\n",
    "    γtrue = zeros(q)\n",
    "    γtrue[1:k] .= rand([-0.2, 0.2], k)\n",
    "    shuffle!(γtrue)\n",
    "    η_G = Gfloat * γtrue\n",
    "\n",
    "    # simulate phenotypes\n",
    "    if y_distribution == Normal\n",
    "        τtrue = 10.0\n",
    "        σ2 = inv(τtrue)\n",
    "        σ = sqrt(σ2)\n",
    "        obs = Vector{GaussianCopulaVCObs{T}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{ContinuousUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i], σ)\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GaussianCopulaVCObs(y, X, V)\n",
    "        end\n",
    "        qc_model = GaussianCopulaVCModel(obs)\n",
    "    else\n",
    "        obs = Vector{GLMCopulaVCObs{T, Dist, Link}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{DiscreteUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i])\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GLMCopulaVCObs(y, X, V, dist, link)\n",
    "        end\n",
    "        qc_model = GLMCopulaVCModel(obs)\n",
    "    end\n",
    "    return qc_model, Γ, G, βtrue, θtrue, γtrue\n",
    "end\n",
    "\n",
    "k = 0 # number of causal SNPs\n",
    "\n",
    "qc_model, Γ, G, βtrue, θtrue, γtrue = simulate_VC_longitudinal(\n",
    "    n = 5000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of fixed effects, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = k, # number of causal SNPs\n",
    "    seed = 1000,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    ")\n",
    "\n",
    "@show qc_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.13.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        5\n",
      "                     variables with only lower bounds:        2\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.5209008e+04 0.00e+00 1.13e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.5204727e+04 0.00e+00 7.65e+01  -2.5 1.13e+02    -  7.17e-03 1.20e-04f  1\n",
      "   2  1.5204686e+04 0.00e+00 7.60e+01 -11.0 2.98e-02    -  1.00e+00 5.98e-03f  1\n",
      "   3  1.5203961e+04 0.00e+00 6.85e+01  -7.4 3.25e-02    -  1.00e+00 1.49e-01f  1\n",
      "   4  1.5202651e+04 0.00e+00 1.30e+02  -4.4 4.89e-02    -  7.72e-01 1.00e+00f  1\n",
      "   5  1.5201283e+04 0.00e+00 4.03e+01  -3.3 1.57e-02    -  1.00e+00 1.00e+00f  1\n",
      "   6  1.5201009e+04 0.00e+00 1.16e+01  -4.4 4.43e-03    -  1.00e+00 1.00e+00f  1\n",
      "   7  1.5200950e+04 0.00e+00 8.84e+00  -5.3 2.73e-03    -  1.00e+00 1.00e+00f  1\n",
      "   8  1.5200580e+04 0.00e+00 1.46e+01  -5.8 2.37e-02    -  1.00e+00 1.00e+00f  1\n",
      "   9  1.5200369e+04 0.00e+00 2.78e+01  -6.4 1.13e-01    -  1.00e+00 2.50e-01f  3\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  1.5199944e+04 0.00e+00 8.05e+00  -6.8 1.52e-01    -  1.00e+00 5.00e-01f  2\n",
      "  11  1.5199900e+04 0.00e+00 5.37e+00  -7.2 8.27e-02    -  1.00e+00 6.25e-02f  5\n",
      "  12  1.5199873e+04 0.00e+00 5.29e+00 -11.0 1.41e-02    -  1.00e+00 1.00e+00f  1\n",
      "  13  1.5199866e+04 0.00e+00 7.02e-01  -9.9 4.31e-03    -  1.00e+00 1.00e+00f  1\n",
      "  14  1.5199866e+04 0.00e+00 9.09e-01 -11.0 2.08e-03    -  1.00e+00 2.50e-01f  3\n",
      "  15  1.5199866e+04 0.00e+00 1.96e-01 -11.0 2.92e-04    -  1.00e+00 1.00e+00f  1\n",
      "  16  1.5199866e+04 0.00e+00 6.39e-03 -11.0 2.57e-04    -  1.00e+00 1.00e+00f  1\n",
      "  17  1.5199866e+04 0.00e+00 1.34e-02 -11.0 2.20e-04    -  1.00e+00 5.00e-01f  2\n",
      "  18  1.5199866e+04 0.00e+00 8.52e-03 -11.0 4.07e-05    -  1.00e+00 6.25e-02f  5\n",
      "  19  1.5199866e+04 0.00e+00 2.15e-03 -11.0 5.54e-06    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20  1.5199866e+04 0.00e+00 3.73e-04 -11.0 9.71e-07    -  1.00e+00 1.00e+00f  1\n",
      "  21  1.5199866e+04 0.00e+00 2.63e-04 -11.0 4.01e-07    -  1.00e+00 1.00e+00f  1\n",
      "  22  1.5199866e+04 0.00e+00 1.40e-05 -11.0 1.65e-07    -  1.00e+00 1.00e+00f  1\n",
      "  23  1.5199866e+04 0.00e+00 1.23e-06 -11.0 3.06e-08    -  1.00e+00 1.00e+00f  1\n",
      "  24  1.5199866e+04 0.00e+00 2.15e-06 -11.0 3.71e-09    -  1.00e+00 1.00e+00f  1\n",
      "  25  1.5199866e+04 0.00e+00 3.20e-07 -11.0 3.40e-09    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 25\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   7.2741929295524114e+03    1.5199865562053361e+04\n",
      "Dual infeasibility......:   3.2034237882504849e-07    6.6937475251549461e-07\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   9.9999999999999962e-12    2.0895604102417751e-11\n",
      "Overall NLP error.......:   3.2034237882504849e-07    6.6937475251549461e-07\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 64\n",
      "Number of objective gradient evaluations             = 26\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      1.866\n",
      "Total CPU secs in NLP function evaluations           =      0.353\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      " 16.235838 seconds (51.73 M allocations: 2.605 GiB, 2.40% gc time, 97.04% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time optm = QuasiCopula.fit!(qc_model,\n",
    "    Ipopt.IpoptSolver(\n",
    "        print_level = 5, \n",
    "        tol = 10^-6, \n",
    "        max_iter = 1000,\n",
    "        accept_after_max_steps = 4,\n",
    "        warm_start_init_point=\"yes\", \n",
    "        limited_memory_max_history = 6, # default value\n",
    "        hessian_approximation = \"limited-memory\",\n",
    "#         derivative_test=\"second-order\"\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βtrue = [1.0, 0.03, 0.03]\n",
      "qc_model.β = [1.0114777784855657, 0.02482165570715728, 0.006462230525953184]\n",
      "qc_model.∇β = [-1.224434083013648e-7, 6.693747525154947e-7, -3.092028739212077e-7]\n",
      "θtrue = [0.1, 0.1]\n",
      "qc_model.θ = [0.0973239094129173, 0.12418281619873915]\n",
      "qc_model.∇θ = [3.02541980801152e-7, 9.500306119569757e-8]\n"
     ]
    }
   ],
   "source": [
    "@show βtrue\n",
    "@show qc_model.β\n",
    "@show qc_model.∇β\n",
    "\n",
    "@show θtrue\n",
    "@show qc_model.θ\n",
    "@show qc_model.∇θ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is $\\nabla_\\beta res$ calculated correctly? \n",
    "\n",
    "We can check using ForwardDiff\n",
    "\n",
    "The function is \n",
    "\n",
    "$$res_{ij}(\\beta) = \\frac{y_{ij} - \\mu_{ij}}{\\sqrt{\\sigma_{ij}^2(\\beta)}}$$\n",
    "\n",
    "### Normal\n",
    "\n",
    "Assumes y, X are given. We calculate the residuals for just 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [-1.5263015405222384, -2.6945001310537258, -1.9847678519577736, -0.900074590336336]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       "  2.07458     2.07458\n",
       " -1.94686    -1.94686\n",
       "  0.0808759   0.0808759\n",
       "  0.154606    0.154606\n",
       " -0.931964   -0.931964\n",
       " -2.26098    -2.26098\n",
       " -1.19819    -1.19819\n",
       "  0.0763038   0.0763038"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(IdentityLink(), η)\n",
    "    varμ = GLM.glmvar.(Normal(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    for i in 1:p, j in 1:d\n",
    "        ∇resβ[j, i] = -X[j, i]\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(qc_model.β) = [0.614284473130316, 0.5863886158923909, 0.60381428358954, 0.6033000519453668, 0.5941433563556224]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.307142   -0.307142\n",
       " -0.293194   -0.293194\n",
       " -0.301907   -0.301907\n",
       " -0.30165    -0.30165\n",
       " -0.297072   -0.297072\n",
       "  0.63719     0.63719\n",
       " -0.570809   -0.570809\n",
       "  0.024417    0.024417\n",
       "  0.046637    0.046637\n",
       " -0.27686    -0.27686\n",
       " -0.694442   -0.694442\n",
       " -0.351303   -0.351303\n",
       "  0.0230367   0.0230367\n",
       " -0.141952   -0.141952\n",
       " -0.3061     -0.3061"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β::AbstractVector{T}) where T\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - \n",
    "            (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# check objective\n",
    "@show resβ(qc_model.β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [0.8012638765796852, -6.734952547066679, -1.3994071698413866, -0.48023808695797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.07727    -1.07727\n",
       " -3.65238    -3.65238\n",
       " -1.22049    -1.22049\n",
       " -1.02842    -1.02842\n",
       "  2.23488     2.23488\n",
       " -7.11068    -7.11068\n",
       "  0.0987079   0.0987079\n",
       "  0.159001    0.159001\n",
       " -1.00397    -1.00397\n",
       " -8.25796    -8.25796\n",
       " -1.46237    -1.46237\n",
       "  0.0784727   0.0784727"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Poisson(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(LogLink(), η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        ∇resβ[j, i] = x_ji * (-(inv(sqrt(varμ_j)) + (0.5 * inv(varμ_j)) * res_j) * dμ_j)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autodiff_loglikelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function A_mul_b!(c::AbstractVector{T}, A::AbstractMatrix, b::AbstractVector) where T\n",
    "    n, p = size(A)\n",
    "    fill!(c, zero(T))\n",
    "    for j in 1:p, i in 1:n\n",
    "        c[i] += A[i, j] * b[j]\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "    ) where T\n",
    "    θ = qc_model.θ\n",
    "    # allocate vector of type T\n",
    "    n, p = size(qc_model.data[1].X)\n",
    "    η = zeros(T, n)\n",
    "    μ = zeros(T, n)\n",
    "    varμ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for gc in qc_model.data\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(η, X, β)\n",
    "        for i in 1:gc.n\n",
    "            μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "            varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             w2[i] = w1[i] * dμ[i]\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        for j in eachindex(y)\n",
    "            res[j] /= sqrt(varμ[j])\n",
    "        end\n",
    "        # std_res_differential! step (this will compute ∇resβ)\n",
    "#         for i in 1:gc.p\n",
    "#             for j in 1:gc.n\n",
    "#                 ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "#             end\n",
    "#         end\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        # component_loglikelihood\n",
    "        for j in 1:gc.n\n",
    "            logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], one(T), one(T))\n",
    "        end\n",
    "        tsum = dot(θ, gc.t)\n",
    "        logl += -log(1 + tsum)\n",
    "        qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    gcm::GaussianCopulaVCModel\n",
    "    ) where T\n",
    "    θ = gcm.θ\n",
    "    τ = gcm.τ[1]\n",
    "    # allocate vector of type T\n",
    "    n, p = size(gcm.data[1].X)\n",
    "    μ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for gc in gcm.data\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        sqrtτ = sqrt(abs(τ))\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(μ, X, β)\n",
    "        for i in 1:gc.n\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        res .*= sqrtτ\n",
    "        rss  = abs2(norm(res)) # RSS of standardized residual\n",
    "        tsum = dot(abs.(θ), gc.t) # ben: why is there abs here?\n",
    "        logl += - log(1 + tsum) - (gc.n * log(2π) -  gc.n * log(abs(τ)) + rss) / 2\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        qsum  = dot(θ, q)\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "# sample data\n",
    "autodiff_loglikelihood(β) = loglikelihood(β, qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.224433049396012e-7\n",
       "  6.693747398034411e-7\n",
       " -3.0920281643420644e-7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.224434083013648e-7\n",
       "  6.693747525154947e-7\n",
       " -3.092028739212077e-7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta^2 L$\n",
    "\n",
    "Hessians for a single observation seems to differ quite a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function two_term_Hessian(gcm::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function two_term_Hessian(gcm::GaussianCopulaVCModel)\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = -sqrt(gcm.τ[1]) .* gc.X # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function three_term_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "#     # sarah's implementation\n",
    "#     loglikelihood!(qc_model, true, true)\n",
    "#     return qc_model.Hβ\n",
    "#     @show qc_model.Hβ\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in qc_model.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "# autodiff ∇²resβ (giving some kind of tensor)\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# this is d²rᵢₖ(β) needed for computing the 4th hessian term\n",
    "function r_ik(β, k)\n",
    "    res = resβ(β)\n",
    "    return res[k]\n",
    "end\n",
    "r_ik(β) = r_ik(β, k)\n",
    "∇²r_ik = x -> ForwardDiff.hessian(r_ik, x)\n",
    "\n",
    "function full_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)    \n",
    "    # loop over samples\n",
    "    for (i, gc) in enumerate(qc_model.data)\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "        # 4th term\n",
    "        ek = zeros(d)    \n",
    "        for k in 1:d\n",
    "            # somehow need to define autodiff functions here, or else k is treated as fixed\n",
    "            function resβ(β)\n",
    "                η = X * β # d by 1\n",
    "                μ = GLM.linkinv.(LogitLink(), η)\n",
    "                varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "                return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "            end\n",
    "            r_ik(β, k) = resβ(β)[k]\n",
    "            r_ik(β) = r_ik(β, k)\n",
    "            ∇²r_ik = x -> ForwardDiff.hessian(r_ik, x) \n",
    "            \n",
    "            \n",
    "            fill!(ek, 0)\n",
    "            ek[k] = 1\n",
    "            X = gc.X\n",
    "            y = gc.y\n",
    "#             @show ∇²r_ik(qc_model.β)\n",
    "            H += (ek' * Γ * res * ∇²r_ik(qc_model.β)) / denom\n",
    "        end\n",
    "    end\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×3 Matrix{Float64}:\n",
       " -0.253406    0.525711    -0.572947\n",
       " -0.279741   -0.544617    -0.335183\n",
       " -0.263081    0.0212769    0.0200741\n",
       " -0.263562    0.0407485   -0.124028\n",
       " -0.272238   -0.253716    -0.280512\n",
       "  0.525711   -1.09063      1.18862\n",
       " -0.544617   -1.06029     -0.652555\n",
       "  0.0212769  -0.00172079  -0.00162351\n",
       "  0.0407485  -0.00629998   0.0191756\n",
       " -0.253716   -0.236454    -0.261427\n",
       " -0.572947    1.18862     -1.29542\n",
       " -0.335183   -0.652555    -0.401613\n",
       "  0.0200741  -0.00162351  -0.00153173\n",
       " -0.124028    0.0191756   -0.0583658\n",
       " -0.280512   -0.261427    -0.289037"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directly evaluating ∇²resβ (giving some kind of tensor)\n",
    "Hββ = ∇²resβ_autodiff(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.253406   0.525711  -0.572947\n",
       "  0.525711  -1.09063    1.18862\n",
       " -0.572947   1.18862   -1.29542"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 1\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.279741  -0.544617  -0.335183\n",
       " -0.544617  -1.06029   -0.652555\n",
       " -0.335183  -0.652555  -0.401613"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 2\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -5800.35       28.5649     84.1043\n",
       "    28.5649  -5155.84       59.5103\n",
       "    84.1043     59.5103  -5164.75"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term Hessian from math\n",
    "two_terms_H = two_term_Hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian from math\n",
    "three_term_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian implemented by sarah\n",
    "loglikelihood!(qc_model, true, true)\n",
    "qc_model.Hβ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 term Hessian from math\n",
    "full_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(\n",
    "        autodiff_loglikelihood, x)\n",
    "autodiff_H = ∇²logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check $\\nabla_{\\theta}L$, $\\nabla^2_{\\theta}L$, and $\\nabla_{\\theta}\\nabla_{\\beta} L$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#22 (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loglikelihood function friendly to autodiff\n",
    "autodiff_loglikelihood(β) = QuasiCopula.loglikelihood(β, qc_model, z)\n",
    "\n",
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "\n",
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(autodiff_loglikelihood, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if `autodiff_loglikelihood` returns same answer as `QuasiCopula.loglikelihood!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff_loglikelihood(fullβ) = -18033.163626812184\n",
      "QuasiCopula.loglikelihood!(qc_model, false, false) = -18033.16362681217\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "# fullβ = [qc_model.β; qc_model.θ; qc_model.τ; 0.0] # normal\n",
    "\n",
    "@show autodiff_loglikelihood(fullβ)\n",
    "@show QuasiCopula.loglikelihood!(qc_model, false, false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.1370986533992892e-6\n",
       "  2.3139215938341664e-6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇logl(fullβ)[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.137098653399271e-6\n",
       "  2.3139215938342303e-6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "grad_math = zeros(m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    grad_math += b / (1 + qc_model.θ'*b) - c / (1 + qc_model.θ'*c)\n",
    "end\n",
    "grad_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta^2 L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 1.25985e-15   1.0245e-15\n",
       " 1.0245e-15   -9.38656e-15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[4:5, 4:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " -1.25985e-15  -1.0245e-15\n",
       " -1.0245e-15    9.38656e-15"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "hess_math = zeros(m, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    hess_math += b*b' / (1 + qc_model.θ'*b)^2 - c*c' / (1 + qc_model.θ'*c)^2\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[1:3, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "p = size(qc_model.data[i].X, 2)\n",
    "hess_math = zeros(p, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    θ = qc_model.θ\n",
    "    ∇resβ = qc_model.data[i].∇resβ\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    A = hcat([∇resβ' * Ω[k] * r for k in 1:m]...)\n",
    "    hess_math += A ./ (1 + θ'*b) - (A*θ ./ (1 + θ'*b)^2) * b'\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check $\\frac{\\partial^2\\mu}{\\partial \\eta^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mueta2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    mueta2(l::Link, η::Real)\n",
    "\n",
    "Second derivative of the inverse link function `d^2μ/dη^2`, for link `L` at linear predictor value `η`.\n",
    "I.e. derivative of the mueta function in GLM.jl\n",
    "\"\"\"\n",
    "function mueta2 end\n",
    "\n",
    "mueta2(::IdentityLink, η::Real) = zero(η)\n",
    "function mueta2(::LogitLink, η::Real)\n",
    "    expabs = exp(-abs(η))\n",
    "    denom = 1 + expabs\n",
    "    return -expabs / denom^2 + 2expabs^2 / denom^3\n",
    "end\n",
    "mueta2(::LogLink, η::Real) = exp(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mueta2(l, η) = -0.015346957645411913\n",
      "mueta2_autodiff(η) = -0.015346957645411843\n"
     ]
    }
   ],
   "source": [
    "# test mueta2 function\n",
    "# l = IdentityLink()\n",
    "# l = LogLink()\n",
    "l = LogitLink()\n",
    "η = 0.1234\n",
    "μ = GLM.linkinv(l, η)\n",
    "\n",
    "# mathematical hessian\n",
    "@show mueta2(l, η)\n",
    "\n",
    "# ForwardDiff Hessian\n",
    "logit_mueta = η -> GLM.mueta(l, η)\n",
    "mueta2_autodiff = x -> ForwardDiff.derivative(logit_mueta, x)\n",
    "@show mueta2_autodiff(η);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial\\sigma^2}{\\partial \\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmamu (generic function with 3 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaeta(D::Distribution, μ::Real)\n",
    "\n",
    "Computes dσ²/dμ\n",
    "\"\"\"\n",
    "function sigmamu end\n",
    "\n",
    "sigmamu(::Normal, μ::Real) = zero(μ)\n",
    "sigmamu(::Bernoulli, μ::Real) = one(μ) - 2μ\n",
    "sigmamu(::Poisson, μ::Real) = one(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial^2(\\sigma^2)}{\\partial \\mu^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaμ2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaμ2(D::Distribution, μ::Real)\n",
    "\n",
    "Computes d²σ²/dμ²\n",
    "\"\"\"\n",
    "function sigmaμ2 end\n",
    "\n",
    "sigmaμ2(::Normal, μ::Real) = zero(μ)\n",
    "sigmaμ2(::Bernoulli, μ::Real) = -2\n",
    "sigmaμ2(::Poisson, μ::Real) = zero(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla \\mu$, $\\nabla^2 \\mu$, $\\nabla \\sigma^2$ and $\\nabla^2 \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ∇²μ(l::Link, X::Matrix, β::Vector)\n",
    "\n",
    "Computes the Hessian of the mean function with respect to β\n",
    "\"\"\"\n",
    "function ∇²μ(l::Link, X::Matrix, β::Vector)\n",
    "    η = X*β\n",
    "    D = Diagonal(mueta2.(l, η))\n",
    "    return X' * D * X\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    ∇²σ(d::Distribution, l::Link, X::Matrix, β::Vector)\n",
    "\n",
    "Computes the Hessian of σ^2 function with respect to β\n",
    "\"\"\"\n",
    "function ∇²σ(d::Distribution, l::Link, X::Matrix, β::Vector)\n",
    "    XXt = X * X'\n",
    "    η = X*β\n",
    "    μ = GLM.linkinv.(l, η)\n",
    "    D = Diagonal(sigmamu2(d, μ)*GLM.mueta.(d, μ).^2 + sigmamu.(d, μ)*mueta2.(l, η))\n",
    "    return X' * D * X\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that we can compute $∇resβ$ generally, although we don't do so in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -1.1211     -1.1211\n",
       " -1.14587    -1.14587\n",
       " -1.12998    -1.12998\n",
       " -1.13043    -1.13043\n",
       " -1.13862    -1.13862\n",
       "  2.3258      2.3258\n",
       " -2.23085    -2.23085\n",
       "  0.0913879   0.0913879\n",
       "  0.174771    0.174771\n",
       " -1.06115    -1.06115\n",
       " -2.53478    -2.53478\n",
       " -1.37297    -1.37297\n",
       "  0.0862215   0.0862215\n",
       " -0.53196    -0.53196\n",
       " -1.17322    -1.17322"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β::AbstractVector{T}) where T\n",
    "    dist = Poisson()\n",
    "    link = LogLink()\n",
    "    \n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(link, η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        dμdβ = dμ_j * x_ji\n",
    "        dσ2dβ = sigmamu(dist, μ_j) * dμdβ\n",
    "        # in practice, we have update_∇resβ fucntions to compute ∇resβ[j, i]\n",
    "        ∇resβ[j, i] = -inv(sqrt(varμ_j)) * dμdβ - 0.5 * res_j * inv(varμ_j) * dσ2dβ\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $∇^2resβ$: Hessian of residual vector of sample $i$ at observation $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "function ∇²resβ_ik()\n",
    "    dist = Poisson()\n",
    "    link = LogLink()\n",
    "    \n",
    "    d, p = size(X)\n",
    "    ∇²resβ_ik = zeros(T, p, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμdη = GLM.mueta.(link, η) # d by 1\n",
    "    for i in 1:p, j in 1:p\n",
    "        varμ_j = varμ[j]\n",
    "        invσ = inv(sqrt(varμ_j))\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμdη_j = dμdη[j]\n",
    "        dμdβ = dμdη_j * x_ji\n",
    "        dσ2dβ = sigmamu(dist, μ_j) * dμdβ\n",
    "        # assemble 5 terms\n",
    "        ∇²resβ_ik[j, i] = -invσ * ?? + \n",
    "                          0.5invσ^3 * dσ2dβ * dμdβ' - \n",
    "                          0.5 * res_j * inv(varμ_j) * ?? + \n",
    "                          0.5invσ^3 * dμdβ * dσ2dβ' + \n",
    "                          0.75res_j * inv(varμ_j^2) * dσ2dβ * dσ2dβ'\n",
    "    end\n",
    "    return ∇²resβ_ik # p × p\n",
    "end\n",
    "\n",
    "# autodiff hessian by computing autodiff gradient of ∇resβ\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
