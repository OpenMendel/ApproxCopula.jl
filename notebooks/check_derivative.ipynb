{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff to check gradient/Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_make_snparray (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using DataFrames, Random, GLM, QuasiCopula\n",
    "using ForwardDiff, Test, LinearAlgebra\n",
    "using LinearAlgebra: BlasReal, copytri!\n",
    "using ToeplitzMatrices\n",
    "using BenchmarkTools\n",
    "using SnpArrays\n",
    "using ForwardDiff\n",
    "# using MendelPlots\n",
    "\n",
    "BLAS.set_num_threads(1)\n",
    "Threads.nthreads()\n",
    "\n",
    "function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "    p::Int64; mafs::Vector{Float64}=zeros(Float64, p), min_ma::Int = 5)\n",
    "\n",
    "    #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "    A1 = BitArray(undef, n, p) \n",
    "    A2 = BitArray(undef, n, p) \n",
    "    for j in 1:p\n",
    "        minor_alleles = 0\n",
    "        maf = 0\n",
    "        while minor_alleles <= min_ma\n",
    "            maf = 0.5rand()\n",
    "            for i in 1:n\n",
    "                A1[i, j] = rand(Bernoulli(maf))\n",
    "                A2[i, j] = rand(Bernoulli(maf))\n",
    "            end\n",
    "            minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "        end\n",
    "        mafs[j] = maf\n",
    "    end\n",
    "\n",
    "    #fill the SnpArray with the corresponding x_tmp entry\n",
    "    return _make_snparray(s, A1, A2)\n",
    "end\n",
    "\n",
    "function _make_snparray(s::Union{String, UndefInitializer}, A1::BitArray, A2::BitArray)\n",
    "    n, p = size(A1)\n",
    "    x = SnpArray(s, n, p)\n",
    "    for i in 1:(n*p)\n",
    "        c = A1[i] + A2[i]\n",
    "        if c == 0\n",
    "            x[i] = 0x00\n",
    "        elseif c == 1\n",
    "            x[i] = 0x02\n",
    "        elseif c == 2\n",
    "            x[i] = 0x03\n",
    "        else\n",
    "            throw(MissingException(\"matrix shouldn't have missing values!\"))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qc_model = Quasi-Copula Variance Component Model\n",
      "  * base distribution: Bernoulli\n",
      "  * link function: LogitLink\n",
      "  * number of clusters: 5000\n",
      "  * cluster size min, max: 5, 5\n",
      "  * number of variance components: 1\n",
      "  * number of fixed effects: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function simulate_VC_longitudinal(;\n",
    "    n = 1000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of nongenetic covariates, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = 10, # number of causal SNPs\n",
    "    seed = 2022,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    )\n",
    "    m == 1 || m == 2 || error(\"m (number of VC) must be 1 or 2\")\n",
    "    \n",
    "    # non-genetic effect sizes\n",
    "    Random.seed!(seed)\n",
    "    βtrue = rand(Uniform(-0.2, 0.2), p)\n",
    "    dist = y_distribution()\n",
    "    link = canonicallink(dist)\n",
    "    Dist = typeof(dist)\n",
    "    Link = typeof(link)\n",
    "\n",
    "    # variance components\n",
    "    θtrue = fill(0.1, m)\n",
    "    V1 = ones(d, d)\n",
    "    V2 = Matrix(I, d, d)\n",
    "    Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "\n",
    "    # simulate design matrices\n",
    "    Random.seed!(seed)\n",
    "    X_full = [hcat(ones(d), randn(d, p - 1)) for i in 1:n]\n",
    "\n",
    "    # simulate random SnpArray with 100 SNPs and randomly choose k SNPs to be causal\n",
    "    Random.seed!(2022)\n",
    "    G = simulate_random_snparray(undef, n, q)\n",
    "    Gfloat = convert(Matrix{T}, G, center=true, scale=false)\n",
    "    γtrue = zeros(q)\n",
    "    γtrue[1:k] .= rand([-0.2, 0.2], k)\n",
    "    shuffle!(γtrue)\n",
    "    η_G = Gfloat * γtrue\n",
    "\n",
    "    # simulate phenotypes\n",
    "    if y_distribution == Normal\n",
    "        τtrue = 10.0\n",
    "        σ2 = inv(τtrue)\n",
    "        σ = sqrt(σ2)\n",
    "        obs = Vector{GaussianCopulaVCObs{T}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{ContinuousUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i], σ)\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GaussianCopulaVCObs(y, X, V)\n",
    "        end\n",
    "        qc_model = GaussianCopulaVCModel(obs)\n",
    "    else\n",
    "        obs = Vector{GLMCopulaVCObs{T, Dist, Link}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{DiscreteUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i])\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GLMCopulaVCObs(y, X, V, dist, link)\n",
    "        end\n",
    "        qc_model = GLMCopulaVCModel(obs)\n",
    "    end\n",
    "    return qc_model, Γ, G, βtrue, θtrue, γtrue\n",
    "end\n",
    "\n",
    "k = 0 # number of causal SNPs\n",
    "\n",
    "qc_model, Γ, G, βtrue, θtrue, γtrue = simulate_VC_longitudinal(\n",
    "    n = 5000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of fixed effects, including intercept\n",
    "    m = 1, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = k, # number of causal SNPs\n",
    "    seed = 1000,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    ")\n",
    "\n",
    "@show qc_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing β using Newton's Algorithm under Independence Assumption\n",
      "gcm.β = [0.012856084500578055, 0.004855553143028062, 0.049012922077609024]\n",
      "initializing variance components using MM-Algorithm\n",
      "gcm.θ = [0.0903852936011474]\n",
      "This is Ipopt version 3.13.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        1\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.7198370e+04 0.00e+00 1.53e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "   1  1.9989060e+04 0.00e+00 5.75e+03 -10.0 1.53e+01    -  1.00e+00 5.97e-02h  5\n",
      "   2  1.7199168e+04 0.00e+00 1.19e+02  -4.2 9.00e-01    -  1.00e+00 1.00e+00f  1\n",
      "   3  1.7199133e+04 0.00e+00 1.18e+02  -4.2 7.96e-02    -  1.00e+00 2.50e-01f  3\n",
      "   4  1.7198333e+04 0.00e+00 7.38e+00  -4.2 1.02e-02    -  1.00e+00 1.00e+00f  1\n",
      "   5  1.7198332e+04 0.00e+00 8.65e-01  -6.3 3.88e-04    -  1.00e+00 1.00e+00f  1\n",
      "   6  1.7198332e+04 0.00e+00 1.14e-01  -8.0 5.08e-05    -  1.00e+00 1.00e+00f  1\n",
      "   7  1.7198332e+04 0.00e+00 6.40e-02  -9.8 1.33e-05    -  1.00e+00 1.00e+00f  1\n",
      "   8  1.7198332e+04 0.00e+00 6.60e-03 -11.0 1.36e-05    -  1.00e+00 1.00e+00f  1\n",
      "   9  1.7198332e+04 0.00e+00 2.29e-03 -11.0 1.82e-06    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  1.7198332e+04 0.00e+00 6.98e-04 -11.0 2.46e-07    -  1.00e+00 1.00e+00f  1\n",
      "  11  1.7198332e+04 0.00e+00 3.50e-04 -11.0 4.12e-08    -  1.00e+00 5.00e-01f  2\n",
      "  12  1.7198332e+04 0.00e+00 2.40e-06 -11.0 2.08e-08    -  1.00e+00 1.00e+00f  1\n",
      "  13  1.7198332e+04 0.00e+00 2.11e-06 -11.0 2.15e-10    -  1.00e+00 2.50e-01f  3\n",
      "  14  1.7198332e+04 0.00e+00 4.63e-08 -11.0 3.92e-10    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 14\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.7198331802305565e+04    1.7198331802305565e+04\n",
      "Dual infeasibility......:   4.6281332875472425e-08    4.6281332875472425e-08\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   9.9999999999999994e-12    9.9999999999999994e-12\n",
      "Overall NLP error.......:   4.6281332875472425e-08    4.6281332875472425e-08\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 36\n",
      "Number of objective gradient evaluations             = 15\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.215\n",
      "Total CPU secs in NLP function evaluations           =      0.196\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "  0.431853 seconds (1.15 M allocations: 32.044 MiB, 42.62% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time optm = QuasiCopula.fit!(qc_model,\n",
    "    Ipopt.IpoptSolver(\n",
    "        print_level = 5, \n",
    "        tol = 10^-6, \n",
    "        max_iter = 1000,\n",
    "        accept_after_max_steps = 4,\n",
    "        warm_start_init_point=\"yes\", \n",
    "        limited_memory_max_history = 6, # default value\n",
    "        hessian_approximation = \"limited-memory\",\n",
    "#         derivative_test=\"second-order\"\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βtrue = [0.019926508247760877, 0.010309257172965214, 0.05956897900629837]\n",
      "qc_model.β = [0.015567602562558885, 0.004040267621651801, 0.05181521692625306]\n",
      "qc_model.∇β = [-1.1793769072454552e-8, 1.724461429208901e-8, -4.6281332875472425e-8]\n",
      "θtrue = [0.1]\n",
      "qc_model.θ = [0.09042550920170454]\n",
      "qc_model.∇θ = [4.175548773410753e-8]\n"
     ]
    }
   ],
   "source": [
    "@show βtrue\n",
    "@show qc_model.β\n",
    "@show qc_model.∇β\n",
    "\n",
    "@show θtrue\n",
    "@show qc_model.θ\n",
    "@show qc_model.∇θ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is $\\nabla_\\beta res$ calculated correctly? \n",
    "\n",
    "We can check using ForwardDiff\n",
    "\n",
    "The function is \n",
    "\n",
    "$$res_{ij}(\\beta) = \\frac{y_{ij} - \\mu_{ij}}{\\sqrt{\\sigma_{ij}^2(\\beta)}}$$\n",
    "\n",
    "### Normal\n",
    "\n",
    "Assumes y, X are given. We calculate the residuals for just 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [-1.5263015405222384, -2.6945001310537258, -1.9847678519577736, -0.900074590336336]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       "  2.07458     2.07458\n",
       " -1.94686    -1.94686\n",
       "  0.0808759   0.0808759\n",
       "  0.154606    0.154606\n",
       " -0.931964   -0.931964\n",
       " -2.26098    -2.26098\n",
       " -1.19819    -1.19819\n",
       "  0.0763038   0.0763038"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(IdentityLink(), η)\n",
    "    varμ = GLM.glmvar.(Normal(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    for i in 1:p, j in 1:d\n",
    "        ∇resβ[j, i] = -X[j, i]\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [1.3943163199970943, 0.24515130649692646, 0.6580775661179072, 0.6577805352369949, 0.390446316792847]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.697158   -0.697158\n",
       " -0.122576   -0.122576\n",
       " -0.329039   -0.329039\n",
       " -0.32889    -0.32889\n",
       " -0.195223   -0.195223\n",
       "  1.44631     1.44631\n",
       " -0.238638   -0.238638\n",
       "  0.0266113   0.0266113\n",
       "  0.0508486   0.0508486\n",
       " -0.181941   -0.181941\n",
       " -1.57626    -1.57626\n",
       " -0.146869   -0.146869\n",
       "  0.0251069   0.0251069\n",
       " -0.154771   -0.154771\n",
       " -0.201156   -0.201156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [0.8012638765796852, -6.734952547066679, -1.3994071698413866, -0.48023808695797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.07727    -1.07727\n",
       " -3.65238    -3.65238\n",
       " -1.22049    -1.22049\n",
       " -1.02842    -1.02842\n",
       "  2.23488     2.23488\n",
       " -7.11068    -7.11068\n",
       "  0.0987079   0.0987079\n",
       "  0.159001    0.159001\n",
       " -1.00397    -1.00397\n",
       " -8.25796    -8.25796\n",
       " -1.46237    -1.46237\n",
       "  0.0784727   0.0784727"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Poisson(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(LogLink(), η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        ∇resβ[j, i] = x_ji * (-(inv(sqrt(varμ_j)) + (0.5 * inv(varμ_j)) * res_j) * dμ_j)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Gradient of likelihood correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autodiff_loglikelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function A_mul_b!(c::AbstractVector{T}, A::AbstractMatrix, b::AbstractVector) where T\n",
    "    n, p = size(A)\n",
    "    fill!(c, zero(T))\n",
    "    for j in 1:p, i in 1:n\n",
    "        c[i] += A[i, j] * b[j]\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "    ) where T\n",
    "    θ = qc_model.θ\n",
    "    # allocate vector of type T\n",
    "    n, p = size(qc_model.data[1].X)\n",
    "    η = zeros(T, n)\n",
    "    μ = zeros(T, n)\n",
    "    varμ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for gc in qc_model.data\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(η, X, β)\n",
    "        for i in 1:gc.n\n",
    "            μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "            varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             w2[i] = w1[i] * dμ[i]\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        for j in eachindex(y)\n",
    "            res[j] /= sqrt(varμ[j])\n",
    "        end\n",
    "        # std_res_differential! step (this will compute ∇resβ)\n",
    "#         for i in 1:gc.p\n",
    "#             for j in 1:gc.n\n",
    "#                 ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "#             end\n",
    "#         end\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        # component_loglikelihood\n",
    "        for j in 1:gc.n\n",
    "            logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], one(T), one(T))\n",
    "        end\n",
    "        tsum = dot(θ, gc.t)\n",
    "        logl += -log(1 + tsum)\n",
    "        qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "# sample data\n",
    "autodiff_loglikelihood(β) = loglikelihood(β, qc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.1793326093467726e-8\n",
       "  1.724427295402009e-8\n",
       " -4.6281334979692e-8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.1793769072454552e-8\n",
       "  1.724461429208901e-8\n",
       " -4.6281332875472425e-8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Hessian of loglikelihood correct?\n",
    "\n",
    "Hessians for a single observation seems to differ quite a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two_term_Hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term hessian from math\n",
    "function two_term_Hessian(gcm)\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -6553.88      -24.8886     72.158\n",
       "   -24.8886  -6294.53       73.4028\n",
       "    72.158      73.4028  -6294.07"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term Hessian from math\n",
    "two_terms_H = two_term_Hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -4634.79      -15.3138     39.9192\n",
       "   -18.6025  -5903.51       64.686\n",
       "    40.9515     65.6278  -5921.69"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian from math\n",
    "loglikelihood!(qc_model, true, true)\n",
    "three_terms_H = qc_model.Hβ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -4173.84      -21.2542     31.7545\n",
       "   -21.2542  -5444.94       58.3133\n",
       "    31.7545     58.3133  -5466.57"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(autodiff_loglikelihood, x)\n",
    "autodiff_H = ∇²logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.000152603   5.8307e-7    -1.74271e-6\n",
       "  5.8307e-7    -0.000158892  -1.84635e-6\n",
       " -1.74271e-6   -1.84635e-6   -0.000158921"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term inv Hessian from math\n",
    "inv(two_terms_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.000215774   5.43617e-7   -1.44864e-6\n",
       "  6.63655e-7   -0.000169413  -1.84612e-6\n",
       " -1.48484e-6   -1.87378e-6   -0.000168901"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term inv Hessian from math\n",
    "inv(three_terms_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.000239603   9.20482e-7   -1.382e-6\n",
       "  9.20482e-7   -0.000183681  -1.95403e-6\n",
       " -1.382e-6     -1.95403e-6   -0.000182959"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff inv Hessian\n",
    "inv(autodiff_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Matrix{Float64}:\n",
       " -4173.84    -6553.88    -4634.79\n",
       "   -21.2542    -24.8886    -18.6025\n",
       "    31.7545     72.158      40.9515\n",
       "   -21.2542    -24.8886    -15.3138\n",
       " -5444.94    -6294.53    -5903.51\n",
       "    58.3133     73.4028     65.6278\n",
       "    31.7545     72.158      39.9192\n",
       "    58.3133     73.4028     64.686\n",
       " -5466.57    -6294.07    -5921.69"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vec(autodiff_H) vec(two_terms_H) vec(three_terms_H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Matrix{Float64}:\n",
       " -0.000239603  -0.000152603  -0.000215774\n",
       "  9.20482e-7    5.8307e-7     6.63655e-7\n",
       " -1.382e-6     -1.74271e-6   -1.48484e-6\n",
       "  9.20482e-7    5.8307e-7     5.43617e-7\n",
       " -0.000183681  -0.000158892  -0.000169413\n",
       " -1.95403e-6   -1.84635e-6   -1.87378e-6\n",
       " -1.382e-6     -1.74271e-6   -1.44864e-6\n",
       " -1.95403e-6   -1.84635e-6   -1.84612e-6\n",
       " -0.000182959  -0.000158921  -0.000168901"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vec(inv(autodiff_H)) vec(inv(two_terms_H)) vec(inv(three_terms_H))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
