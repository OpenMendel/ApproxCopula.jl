{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff to check gradient/Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling QuasiCopula [c47b6ae2-b804-4668-9957-eb588c99ffbc]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_make_snparray (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using DataFrames, Random, GLM, QuasiCopula\n",
    "using ForwardDiff, Test, LinearAlgebra\n",
    "using LinearAlgebra: BlasReal, copytri!\n",
    "using ToeplitzMatrices\n",
    "using BenchmarkTools\n",
    "using SnpArrays\n",
    "using ForwardDiff\n",
    "# using MendelPlots\n",
    "\n",
    "BLAS.set_num_threads(1)\n",
    "Threads.nthreads()\n",
    "\n",
    "function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "    p::Int64; mafs::Vector{Float64}=zeros(Float64, p), min_ma::Int = 5)\n",
    "\n",
    "    #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "    A1 = BitArray(undef, n, p) \n",
    "    A2 = BitArray(undef, n, p) \n",
    "    for j in 1:p\n",
    "        minor_alleles = 0\n",
    "        maf = 0\n",
    "        while minor_alleles <= min_ma\n",
    "            maf = 0.5rand()\n",
    "            for i in 1:n\n",
    "                A1[i, j] = rand(Bernoulli(maf))\n",
    "                A2[i, j] = rand(Bernoulli(maf))\n",
    "            end\n",
    "            minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "        end\n",
    "        mafs[j] = maf\n",
    "    end\n",
    "\n",
    "    #fill the SnpArray with the corresponding x_tmp entry\n",
    "    return _make_snparray(s, A1, A2)\n",
    "end\n",
    "\n",
    "function _make_snparray(s::Union{String, UndefInitializer}, A1::BitArray, A2::BitArray)\n",
    "    n, p = size(A1)\n",
    "    x = SnpArray(s, n, p)\n",
    "    for i in 1:(n*p)\n",
    "        c = A1[i] + A2[i]\n",
    "        if c == 0\n",
    "            x[i] = 0x00\n",
    "        elseif c == 1\n",
    "            x[i] = 0x02\n",
    "        elseif c == 2\n",
    "            x[i] = 0x03\n",
    "        else\n",
    "            throw(MissingException(\"matrix shouldn't have missing values!\"))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qc_model = Quasi-Copula Variance Component Model\n",
      "  * base distribution: Bernoulli\n",
      "  * link function: LogitLink\n",
      "  * number of clusters: 5000\n",
      "  * cluster size min, max: 5, 5\n",
      "  * number of variance components: 1\n",
      "  * number of fixed effects: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function simulate_VC_longitudinal(;\n",
    "    n = 1000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of nongenetic covariates, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = 10, # number of causal SNPs\n",
    "    seed = 2022,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    )\n",
    "    m == 1 || m == 2 || error(\"m (number of VC) must be 1 or 2\")\n",
    "    \n",
    "    # non-genetic effect sizes\n",
    "    Random.seed!(seed)\n",
    "    βtrue = rand(Uniform(-0.2, 0.2), p)\n",
    "    dist = y_distribution()\n",
    "    link = canonicallink(dist)\n",
    "    Dist = typeof(dist)\n",
    "    Link = typeof(link)\n",
    "\n",
    "    # variance components\n",
    "    θtrue = fill(0.1, m)\n",
    "    V1 = ones(d, d)\n",
    "    V2 = Matrix(I, d, d)\n",
    "    Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "\n",
    "    # simulate design matrices\n",
    "    Random.seed!(seed)\n",
    "    X_full = [hcat(ones(d), randn(d, p - 1)) for i in 1:n]\n",
    "\n",
    "    # simulate random SnpArray with 100 SNPs and randomly choose k SNPs to be causal\n",
    "    Random.seed!(2022)\n",
    "    G = simulate_random_snparray(undef, n, q)\n",
    "    Gfloat = convert(Matrix{T}, G, center=true, scale=false)\n",
    "    γtrue = zeros(q)\n",
    "    γtrue[1:k] .= rand([-0.2, 0.2], k)\n",
    "    shuffle!(γtrue)\n",
    "    η_G = Gfloat * γtrue\n",
    "\n",
    "    # simulate phenotypes\n",
    "    if y_distribution == Normal\n",
    "        τtrue = 10.0\n",
    "        σ2 = inv(τtrue)\n",
    "        σ = sqrt(σ2)\n",
    "        obs = Vector{GaussianCopulaVCObs{T}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{ContinuousUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i], σ)\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GaussianCopulaVCObs(y, X, V)\n",
    "        end\n",
    "        qc_model = GaussianCopulaVCModel(obs)\n",
    "    else\n",
    "        obs = Vector{GLMCopulaVCObs{T, Dist, Link}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{DiscreteUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i])\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GLMCopulaVCObs(y, X, V, dist, link)\n",
    "        end\n",
    "        qc_model = GLMCopulaVCModel(obs)\n",
    "    end\n",
    "    return qc_model, Γ, G, βtrue, θtrue, γtrue\n",
    "end\n",
    "\n",
    "k = 0 # number of causal SNPs\n",
    "\n",
    "qc_model, Γ, G, βtrue, θtrue, γtrue = simulate_VC_longitudinal(\n",
    "    n = 5000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of fixed effects, including intercept\n",
    "    m = 1, # number of variance components\n",
    "    q = 0, # number of SNPs\n",
    "    k = k, # number of causal SNPs\n",
    "    seed = 1000,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    ")\n",
    "\n",
    "@show qc_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing β using Newton's Algorithm under Independence Assumption\n",
      "gcm.β = [0.026499669081559256, -0.011594461794217123, 0.06008621943931356]\n",
      "initializing variance components using MM-Algorithm\n",
      "gcm.θ = [0.10572650177558549]\n",
      "This is Ipopt version 3.13.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        1\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.7161794e+04 0.00e+00 2.53e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "   1  2.2045463e+04 0.00e+00 6.93e+03 -10.0 2.53e+01    -  1.00e+00 6.25e-02h  5\n",
      "   2  1.7164394e+04 0.00e+00 1.70e+02  -4.2 1.55e+00    -  1.00e+00 1.00e+00f  1\n",
      "   3  1.7161812e+04 0.00e+00 2.48e+01  -4.2 2.40e-01    -  1.00e+00 1.18e-01f  4\n",
      "   4  1.7161733e+04 0.00e+00 3.44e+00  -4.2 4.65e-03    -  1.00e+00 1.00e+00f  1\n",
      "   5  1.7161732e+04 0.00e+00 3.20e-01  -6.3 2.15e-04    -  1.00e+00 1.00e+00f  1\n",
      "   6  1.7161732e+04 0.00e+00 1.19e-02  -8.1 2.19e-05    -  1.00e+00 1.00e+00f  1\n",
      "   7  1.7161732e+04 0.00e+00 5.50e-03 -10.0 1.21e-06    -  1.00e+00 1.00e+00f  1\n",
      "   8  1.7161732e+04 0.00e+00 4.49e-04 -10.1 1.06e-06    -  1.00e+00 1.00e+00f  1\n",
      "   9  1.7161732e+04 0.00e+00 4.03e-05 -11.0 1.03e-07    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  1.7161732e+04 0.00e+00 5.72e-07 -11.0 1.02e-08    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 10\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.7161732336726433e+04    1.7161732336726433e+04\n",
      "Dual infeasibility......:   5.7183907991159587e-07    5.7183907991159587e-07\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   9.9999999999999816e-12    9.9999999999999816e-12\n",
      "Overall NLP error.......:   5.7183907991159587e-07    5.7183907991159587e-07\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 22\n",
      "Number of objective gradient evaluations             = 11\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.024\n",
      "Total CPU secs in NLP function evaluations           =      0.133\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "  0.176494 seconds (526.08 k allocations: 11.653 MiB, 9.61% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time optm = QuasiCopula.fit!(qc_model,\n",
    "    Ipopt.IpoptSolver(\n",
    "        print_level = 5, \n",
    "        tol = 10^-6, \n",
    "        max_iter = 1000,\n",
    "        accept_after_max_steps = 4,\n",
    "        warm_start_init_point=\"yes\", \n",
    "        limited_memory_max_history = 6, # default value\n",
    "        hessian_approximation = \"limited-memory\",\n",
    "#         derivative_test=\"second-order\"\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βtrue = [0.019926508247760877, 0.010309257172965214, 0.05956897900629837]\n",
      "qc_model.β = [0.02649195678310015, -0.010667144141669664, 0.06480115759419805]\n",
      "qc_model.∇β = [1.0586084292496523e-7, -5.6072888887648276e-8, -4.622918350072336e-7]\n",
      "θtrue = [0.1]\n",
      "qc_model.θ = [0.10577520258371464]\n",
      "qc_model.∇θ = [-5.719336200193936e-7]\n"
     ]
    }
   ],
   "source": [
    "@show βtrue\n",
    "@show qc_model.β\n",
    "@show qc_model.∇β\n",
    "\n",
    "@show θtrue\n",
    "@show qc_model.θ\n",
    "@show qc_model.∇θ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is $\\nabla_\\beta res$ calculated correctly? \n",
    "\n",
    "We can check using ForwardDiff\n",
    "\n",
    "The function is \n",
    "\n",
    "$$res_{ij}(\\beta) = \\frac{y_i - \\mu_i}{\\sigma_{ij}(\\beta)}$$\n",
    "\n",
    "### Normal\n",
    "\n",
    "Assumes y, X are given. We calculate the residuals for just 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [-1.5263015405222384, -2.6945001310537258, -1.9847678519577736, -0.900074590336336]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       "  2.07458     2.07458\n",
       " -1.94686    -1.94686\n",
       "  0.0808759   0.0808759\n",
       "  0.154606    0.154606\n",
       " -0.931964   -0.931964\n",
       " -2.26098    -2.26098\n",
       " -1.19819    -1.19819\n",
       "  0.0763038   0.0763038"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(IdentityLink(), η)\n",
    "    varμ = GLM.glmvar.(Normal(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    for i in 1:p, j in 1:d\n",
    "        ∇resβ[j, i] = -X[j, i]\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [1.3943163199970943, 0.24515130649692646, 0.6580775661179072, 0.6577805352369949, 0.390446316792847]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.697158   -0.697158\n",
       " -0.122576   -0.122576\n",
       " -0.329039   -0.329039\n",
       " -0.32889    -0.32889\n",
       " -0.195223   -0.195223\n",
       "  1.44631     1.44631\n",
       " -0.238638   -0.238638\n",
       "  0.0266113   0.0266113\n",
       "  0.0508486   0.0508486\n",
       " -0.181941   -0.181941\n",
       " -1.57626    -1.57626\n",
       " -0.146869   -0.146869\n",
       "  0.0251069   0.0251069\n",
       " -0.154771   -0.154771\n",
       " -0.201156   -0.201156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [0.8012638765796852, -6.734952547066679, -1.3994071698413866, -0.48023808695797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12×2 Matrix{Float64}:\n",
       " -1.07727    -1.07727\n",
       " -3.65238    -3.65238\n",
       " -1.22049    -1.22049\n",
       " -1.02842    -1.02842\n",
       "  2.23488     2.23488\n",
       " -7.11068    -7.11068\n",
       "  0.0987079   0.0987079\n",
       "  0.159001    0.159001\n",
       " -1.00397    -1.00397\n",
       " -8.25796    -8.25796\n",
       " -1.46237    -1.46237\n",
       "  0.0784727   0.0784727"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Poisson(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(LogLink(), η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        ∇resβ[j, i] = x_ji * (-(inv(sqrt(varμ_j)) + (0.5 * inv(varμ_j)) * res_j) * dμ_j)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Gradient of likelihood correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.0570025767760889\n",
       " -2.218682510700683\n",
       "  0.18209959353709226"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "gc = qc_model.data[1]\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "θ = qc_model.θ\n",
    "\n",
    "function A_mul_b!(c::AbstractVector{T}, A::AbstractMatrix, b::AbstractVector) where T\n",
    "    n, p = size(A)\n",
    "    fill!(c, zero(T))\n",
    "    for j in 1:p, i in 1:n\n",
    "        c[i] += A[i, j] * b[j]\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "# assumes y, X are available\n",
    "function autodiff_loglikelihood_bernoulli(β::AbstractVector{T}) where T\n",
    "    n, p = size(X)\n",
    "    # allocate vector of type T\n",
    "    η = zeros(T, n)\n",
    "    μ = zeros(T, n)\n",
    "    varμ = zeros(T, n)\n",
    "    dμ = zeros(T, n)\n",
    "    w1 = zeros(T, n)\n",
    "    w2 = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    ∇resβ = zeros(T, n, p)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, gc.m)\n",
    "    # update_res! step (need to avoid BLAS)\n",
    "    A_mul_b!(η, X, β)\n",
    "    for i in 1:gc.n\n",
    "        μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "        varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "        dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "        w1[i] = dμ[i] / varμ[i]\n",
    "        w2[i] = w1[i] * dμ[i]\n",
    "        res[i] = y[i] - μ[i]\n",
    "    end\n",
    "    # standardize_res! step\n",
    "    for j in eachindex(y)\n",
    "        res[j] /= sqrt(varμ[j])\n",
    "    end\n",
    "    # std_res_differential! step (this will compute ∇resβ)\n",
    "    for i in 1:gc.p\n",
    "        for j in 1:gc.n\n",
    "            ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "        end\n",
    "    end\n",
    "    # update Γ\n",
    "    @inbounds for k in 1:gc.m\n",
    "        A_mul_b!(storage_n, gc.V[k], res)\n",
    "        q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "    end\n",
    "    # component_loglikelihood\n",
    "    logl = zero(T)\n",
    "    for j in 1:gc.n\n",
    "        logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], one(T), one(T))\n",
    "    end\n",
    "    tsum = dot(θ, gc.t)\n",
    "    logl += -log(1 + tsum)\n",
    "    qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "    logl += log(1 + qsum)\n",
    "    return logl\n",
    "end\n",
    "\n",
    "# autodiff Gradient\n",
    "∇logl_bernoulli = x -> ForwardDiff.gradient(autodiff_loglikelihood_bernoulli, x)\n",
    "∇logl_bernoulli(qc_model.β)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -1.0570025767760889\n",
       " -2.2186825107006833\n",
       "  0.1820995935370926"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood!(qc_model, true, false)\n",
    "qc_model.data[1].∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Hessian of loglikelihood correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Type{T})(::Real, \u001b[91m::RoundingMode\u001b[39m) where T<:AbstractFloat at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/rounding.jl:200\n\u001b[0m  (::Type{T})(::T) where T<:Number at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/boot.jl:770\n\u001b[0m  (::Type{T})(\u001b[91m::VectorizationBase.Double{T}\u001b[39m) where T<:Union{Float16, Float32, Float64, VectorizationBase.Vec{<:Any, <:Union{Float16, Float32, Float64}}, VectorizationBase.VecUnroll{var\"#s36\", var\"#s35\", var\"#s34\", V} where {var\"#s36\", var\"#s35\", var\"#s34\"<:Union{Float16, Float32, Float64}, V<:Union{Bool, Float16, Float32, Float64, Int16, Int32, Int64, Int8, UInt16, UInt32, UInt64, UInt8, SIMDTypes.Bit, VectorizationBase.AbstractSIMD{var\"#s35\", var\"#s34\"}}}} at ~/.julia/packages/VectorizationBase/oCgEJ/src/special/double.jl:100\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Type{T})(::Real, \u001b[91m::RoundingMode\u001b[39m) where T<:AbstractFloat at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/rounding.jl:200\n\u001b[0m  (::Type{T})(::T) where T<:Number at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/boot.jl:770\n\u001b[0m  (::Type{T})(\u001b[91m::VectorizationBase.Double{T}\u001b[39m) where T<:Union{Float16, Float32, Float64, VectorizationBase.Vec{<:Any, <:Union{Float16, Float32, Float64}}, VectorizationBase.VecUnroll{var\"#s36\", var\"#s35\", var\"#s34\", V} where {var\"#s36\", var\"#s35\", var\"#s34\"<:Union{Float16, Float32, Float64}, V<:Union{Bool, Float16, Float32, Float64, Int16, Int32, Int64, Int8, UInt16, UInt32, UInt64, UInt8, SIMDTypes.Bit, VectorizationBase.AbstractSIMD{var\"#s35\", var\"#s34\"}}}} at ~/.julia/packages/VectorizationBase/oCgEJ/src/special/double.jl:100\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] convert(#unused#::Type{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3})",
      "    @ Base ./number.jl:7",
      "  [2] setindex!(A::Vector{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}, i1::Int64)",
      "    @ Base ./array.jl:903",
      "  [3] _unsafe_copyto!(dest::Vector{Float64}, doffs::Int64, src::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}}, soffs::Int64, n::Int64)",
      "    @ Base ./array.jl:253",
      "  [4] unsafe_copyto!",
      "    @ ./array.jl:307 [inlined]",
      "  [5] _copyto_impl!",
      "    @ ./array.jl:331 [inlined]",
      "  [6] copyto!",
      "    @ ./array.jl:317 [inlined]",
      "  [7] copyto!",
      "    @ ./array.jl:343 [inlined]",
      "  [8] copyto!",
      "    @ ./broadcast.jl:954 [inlined]",
      "  [9] copyto!",
      "    @ ./broadcast.jl:913 [inlined]",
      " [10] materialize!",
      "    @ ./broadcast.jl:871 [inlined]",
      " [11] materialize!(dest::Vector{Float64}, bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(identity), Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}}}})",
      "    @ Base.Broadcast ./broadcast.jl:868",
      " [12] loglikelihood(β::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}})",
      "    @ Main ./In[6]:2",
      " [13] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      " [14] vector_mode_jacobian(f::typeof(loglikelihood), x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:148",
      " [15] jacobian(f::Function, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}}}, ::Val{true})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:21",
      " [16] jacobian(f::Function, x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}, ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}, 3}}}) (repeats 2 times)",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:19",
      " [17] #11",
      "    @ ./In[9]:3 [inlined]",
      " [18] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      " [19] vector_mode_jacobian(f::var\"#11#12\", x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:148",
      " [20] jacobian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}}, ::Val{true})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:21",
      " [21] jacobian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{var\"#11#12\", Float64}, Float64, 3}}}) (repeats 2 times)",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:19",
      " [22] top-level scope",
      "    @ In[9]:2",
      " [23] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [24] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "ForwardDiff.jacobian(\n",
    "    x -> ForwardDiff.jacobian(autodiff_loglikelihood_bernoulli, x), qc_model.β\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Type{T})(::Real, \u001b[91m::RoundingMode\u001b[39m) where T<:AbstractFloat at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/rounding.jl:200\n\u001b[0m  (::Type{T})(::T) where T<:Number at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/boot.jl:770\n\u001b[0m  (::Type{T})(\u001b[91m::VectorizationBase.Double{T}\u001b[39m) where T<:Union{Float16, Float32, Float64, VectorizationBase.Vec{<:Any, <:Union{Float16, Float32, Float64}}, VectorizationBase.VecUnroll{var\"#s36\", var\"#s35\", var\"#s34\", V} where {var\"#s36\", var\"#s35\", var\"#s34\"<:Union{Float16, Float32, Float64}, V<:Union{Bool, Float16, Float32, Float64, Int16, Int32, Int64, Int8, UInt16, UInt32, UInt64, UInt8, SIMDTypes.Bit, VectorizationBase.AbstractSIMD{var\"#s35\", var\"#s34\"}}}} at ~/.julia/packages/VectorizationBase/oCgEJ/src/special/double.jl:100\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Type{T})(::Real, \u001b[91m::RoundingMode\u001b[39m) where T<:AbstractFloat at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/rounding.jl:200\n\u001b[0m  (::Type{T})(::T) where T<:Number at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/boot.jl:770\n\u001b[0m  (::Type{T})(\u001b[91m::VectorizationBase.Double{T}\u001b[39m) where T<:Union{Float16, Float32, Float64, VectorizationBase.Vec{<:Any, <:Union{Float16, Float32, Float64}}, VectorizationBase.VecUnroll{var\"#s36\", var\"#s35\", var\"#s34\", V} where {var\"#s36\", var\"#s35\", var\"#s34\"<:Union{Float16, Float32, Float64}, V<:Union{Bool, Float16, Float32, Float64, Int16, Int32, Int64, Int8, UInt16, UInt32, UInt64, UInt8, SIMDTypes.Bit, VectorizationBase.AbstractSIMD{var\"#s35\", var\"#s34\"}}}} at ~/.julia/packages/VectorizationBase/oCgEJ/src/special/double.jl:100\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] convert(#unused#::Type{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3})",
      "    @ Base ./number.jl:7",
      "  [2] setindex!(A::Vector{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}, i1::Int64)",
      "    @ Base ./array.jl:903",
      "  [3] _unsafe_copyto!(dest::Vector{Float64}, doffs::Int64, src::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}, soffs::Int64, n::Int64)",
      "    @ Base ./array.jl:253",
      "  [4] unsafe_copyto!",
      "    @ ./array.jl:307 [inlined]",
      "  [5] _copyto_impl!",
      "    @ ./array.jl:331 [inlined]",
      "  [6] copyto!",
      "    @ ./array.jl:317 [inlined]",
      "  [7] copyto!",
      "    @ ./array.jl:343 [inlined]",
      "  [8] copyto!",
      "    @ ./broadcast.jl:954 [inlined]",
      "  [9] copyto!",
      "    @ ./broadcast.jl:913 [inlined]",
      " [10] materialize!",
      "    @ ./broadcast.jl:871 [inlined]",
      " [11] materialize!(dest::Vector{Float64}, bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(identity), Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}}})",
      "    @ Base.Broadcast ./broadcast.jl:868",
      " [12] loglikelihood(β::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}})",
      "    @ Main ./In[6]:2",
      " [13] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      " [14] vector_mode_gradient(f::typeof(loglikelihood), x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:106",
      " [15] gradient",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:19 [inlined]",
      " [16] #114",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:16 [inlined]",
      " [17] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      " [18] vector_mode_jacobian(f::ForwardDiff.var\"#114#115\"{typeof(loglikelihood), ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}}}, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:148",
      " [19] jacobian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}}, ::Val{false})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:21",
      " [20] hessian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}}, ::Val{true})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:17",
      " [21] hessian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}, 3}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood), Float64}, Float64, 3}}}) (repeats 2 times)",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:15",
      " [22] (::var\"#9#10\")(x::Vector{Float64})",
      "    @ Main ./In[8]:2",
      " [23] top-level scope",
      "    @ In[8]:3",
      " [24] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [25] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "autodiff_hessian = x -> ForwardDiff.hessian(loglikelihood, x)\n",
    "H_autodiff = autodiff_hessian(qc_model.β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian evaluated with 2 terms only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  7896.6    2695.55  -3169.72\n",
       "  2695.55   8402.23  -3442.68\n",
       " -3169.72  -3442.68   9146.81"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, q = size(G)\n",
    "p = length(qc_model.β)\n",
    "T = eltype(qc_model.data[1].X)\n",
    "P = zeros(T, p, p)\n",
    "for i in 1:n\n",
    "    gc = qc_model.data[i]\n",
    "    d = gc.n # number of observations for current sample\n",
    "    # GLM term\n",
    "    P += Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "    # trailing terms\n",
    "    res = gc.res # d × 1 standardized residuals\n",
    "    ∇resβ = gc.∇resβ # d × p\n",
    "    Γ = zeros(T, d, d)\n",
    "    for k in 1:gc.m # loop over variance components\n",
    "        Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "    end\n",
    "    denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "    P += (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "end\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
