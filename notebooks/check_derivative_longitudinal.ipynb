{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff to check gradient/Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling QuasiCopula [c47b6ae2-b804-4668-9957-eb588c99ffbc]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_make_snparray (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using DataFrames, Random, GLM, QuasiCopula\n",
    "using ForwardDiff, Test, LinearAlgebra\n",
    "using LinearAlgebra: BlasReal, copytri!\n",
    "using ToeplitzMatrices\n",
    "using BenchmarkTools\n",
    "using SnpArrays\n",
    "using ForwardDiff\n",
    "# using MendelPlots\n",
    "ENV[\"COLUMNS\"] = 240\n",
    "\n",
    "BLAS.set_num_threads(1)\n",
    "Threads.nthreads()\n",
    "\n",
    "# simulate PLINK data where each SNP has at least min_ma copies of the alt allele\n",
    "function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "        p::Int64, mafs::Vector{Float64}; min_ma::Int = 1)\n",
    "    all(0.0 .<= mafs .<= 0.5) || throw(ArgumentError(\"vector of minor allele frequencies must be in (0, 0.5)\"))\n",
    "#     any(mafs .<= 0.0005) && @warn(\"Provided minor allele frequencies contain entries smaller than 0.0005, simulation may take long if sample size is small and min_ma = $min_ma is large\")\n",
    "\n",
    "    #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "    A1 = BitArray(undef, n, p) \n",
    "    A2 = BitArray(undef, n, p) \n",
    "    for j in 1:p\n",
    "        minor_alleles = 0\n",
    "        maf = mafs[j]\n",
    "        while minor_alleles <= min_ma\n",
    "            for i in 1:n\n",
    "                A1[i, j] = rand(Bernoulli(maf))\n",
    "                A2[i, j] = rand(Bernoulli(maf))\n",
    "            end\n",
    "            minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #fill the SnpArray with the corresponding x_tmp entry\n",
    "    return _make_snparray(s, A1, A2)\n",
    "end\n",
    "\n",
    "\n",
    "# function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "#     p::Int64; mafs::Vector{Float64}=zeros(Float64, p), min_ma::Int = 5)\n",
    "\n",
    "#     #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "#     A1 = BitArray(undef, n, p) \n",
    "#     A2 = BitArray(undef, n, p) \n",
    "#     for j in 1:p\n",
    "#         minor_alleles = 0\n",
    "#         maf = 0\n",
    "#         while minor_alleles <= min_ma\n",
    "#             maf = 0.5rand()\n",
    "#             for i in 1:n\n",
    "#                 A1[i, j] = rand(Bernoulli(maf))\n",
    "#                 A2[i, j] = rand(Bernoulli(maf))\n",
    "#             end\n",
    "#             minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "#         end\n",
    "#         mafs[j] = maf\n",
    "#     end\n",
    "\n",
    "#     #fill the SnpArray with the corresponding x_tmp entry\n",
    "#     return _make_snparray(s, A1, A2)\n",
    "# end\n",
    "\n",
    "function _make_snparray(s::Union{String, UndefInitializer}, A1::BitArray, A2::BitArray)\n",
    "    n, p = size(A1)\n",
    "    x = SnpArray(s, n, p)\n",
    "    for i in 1:(n*p)\n",
    "        c = A1[i] + A2[i]\n",
    "        if c == 0\n",
    "            x[i] = 0x00\n",
    "        elseif c == 1\n",
    "            x[i] = 0x02\n",
    "        elseif c == 2\n",
    "            x[i] = 0x03\n",
    "        else\n",
    "            throw(MissingException(\"matrix shouldn't have missing values!\"))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qc_model = Quasi-Copula Variance Component Model\n",
      "  * base distribution: Bernoulli\n",
      "  * link function: LogitLink\n",
      "  * number of clusters: 5000\n",
      "  * cluster size min, max: 5, 5\n",
      "  * number of variance components: 2\n",
      "  * number of fixed effects: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function simulate_VC_longitudinal(;\n",
    "    n = 1000, # sample size\n",
    "    d_min = 1, # min number of observations per sample\n",
    "    d_max = 5, # max number of observations per sample\n",
    "    p = 3, # number of nongenetic covariates, including intercept\n",
    "    m = 1, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = 10, # number of causal SNPs\n",
    "    maf = 0.5rand(),\n",
    "    causal_snp_β = 0.5rand(),\n",
    "    seed = 2022,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    )\n",
    "    Random.seed!(seed)\n",
    "    m == 1 || m == 2 || error(\"m (number of VC) must be 1 or 2\")\n",
    "    \n",
    "    # non-genetic effect sizes\n",
    "    Random.seed!(seed)\n",
    "#     βtrue = [1.0; rand(-0.05:0.1:0.05, p-1)]\n",
    "    βtrue = [1.0; rand(-0.5:1:0.5, p-1)]\n",
    "#     βtrue = [1.0; rand(-5:10:5, p-1) .* rand(Uniform(0, 5), p-1)]\n",
    "    dist = y_distribution()\n",
    "    link = canonicallink(dist)\n",
    "    Dist = typeof(dist)\n",
    "    Link = typeof(link)\n",
    "\n",
    "    # variance components\n",
    "    θtrue = fill(0.1, m)\n",
    "\n",
    "    # simulate (nongenetic) design matrices\n",
    "    Random.seed!(seed)\n",
    "    X_full = Matrix{Float64}[]\n",
    "    for i in 1:n\n",
    "        nobs = rand(d_min:d_max) # number of obs for this sample\n",
    "        push!(X_full, hcat(ones(nobs), randn(nobs, p - 1)))\n",
    "    end\n",
    "    \n",
    "    # simulate causal alleles\n",
    "    Random.seed!(seed)\n",
    "    γtrue = zeros(q)\n",
    "#     γtrue[1:k] .= rand([-0.2, 0.2], k)\n",
    "    γtrue[1:k] .= causal_snp_β\n",
    "    shuffle!(γtrue)\n",
    "    \n",
    "    # set minor allele freq\n",
    "    mafs = fill(maf, q)\n",
    "    \n",
    "    # simulate random SnpArray with q SNPs with prespecified maf\n",
    "    Random.seed!(seed)\n",
    "    G = simulate_random_snparray(undef, n, q, mafs)\n",
    "    Gfloat = convert(Matrix{T}, G, center=true, scale=false)\n",
    "    \n",
    "    # effect of causal alleles\n",
    "    η_G = Gfloat * γtrue\n",
    "\n",
    "    # simulate phenotypes\n",
    "    if y_distribution == Normal\n",
    "        τtrue = 10.0\n",
    "        σ2 = inv(τtrue)\n",
    "        σ = sqrt(σ2)\n",
    "        obs = Vector{GaussianCopulaVCObs{T}}(undef, n)\n",
    "        for i in 1:n\n",
    "            # data matrix\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{ContinuousUnivariateDistribution}(undef, size(X, 1))\n",
    "            # VC matrices\n",
    "            V1 = ones(size(X, 1), size(X, 1))\n",
    "            V2 = Matrix(I, size(X, 1), size(X, 1))\n",
    "            Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "            for i in 1:size(X, 1)\n",
    "                vecd[i] = y_distribution(μ[i], σ)\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, size(X, 1))\n",
    "            res = Vector{T}(undef, size(X, 1))\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GaussianCopulaVCObs(y, X, V)\n",
    "        end\n",
    "        qc_model = GaussianCopulaVCModel(obs)\n",
    "    else\n",
    "        obs = Vector{GLMCopulaVCObs{T, Dist, Link}}(undef, n)\n",
    "        for i in 1:n\n",
    "            # data matrix\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            # VC matrices\n",
    "            V1 = ones(size(X, 1), size(X, 1))\n",
    "            V2 = Matrix(I, size(X, 1), size(X, 1))\n",
    "            Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "            vecd = Vector{DiscreteUnivariateDistribution}(undef, size(X, 1))\n",
    "            for i in 1:size(X, 1)\n",
    "                vecd[i] = y_distribution(μ[i])\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, size(X, 1))\n",
    "            res = Vector{T}(undef, size(X, 1))\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GLMCopulaVCObs(y, X, V, dist, link)\n",
    "        end\n",
    "        qc_model = GLMCopulaVCModel(obs)\n",
    "    end\n",
    "    return qc_model, G, βtrue, θtrue, γtrue\n",
    "end\n",
    "\n",
    "k = 0 # number of causal SNPs\n",
    "maf = 0.3\n",
    "\n",
    "qc_model, G, βtrue, θtrue, γtrue = simulate_VC_longitudinal(\n",
    "    n = 5000, # sample size\n",
    "    d_min = 5, # min number of observations per sample\n",
    "    d_max = 5, # max number of observations per sample\n",
    "    p = 3, # number of fixed effects, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = k, # number of causal SNPs\n",
    "    seed = 123,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    maf = maf,\n",
    "    causal_snp_β = 0.2\n",
    ")\n",
    "\n",
    "@show qc_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.13.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        5\n",
      "                     variables with only lower bounds:        2\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.4837198e+04 0.00e+00 1.15e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.4832308e+04 0.00e+00 8.43e+01  -2.4 1.04e+02    -  1.90e-02 9.52e-05f  1\n",
      "   2  1.4832243e+04 0.00e+00 7.31e+01  -7.1 3.02e-02    -  1.00e+00 6.00e-03f  1\n",
      "   3  1.4826779e+04 0.00e+00 3.94e+01  -6.9 4.52e-02    -  1.00e+00 6.06e-01f  1\n",
      "   4  1.4825400e+04 0.00e+00 7.18e+01  -4.5 3.73e-02    -  6.81e-01 5.00e-01f  2\n",
      "   5  1.4823906e+04 0.00e+00 3.85e+01  -2.8 4.50e-02    -  1.68e-02 5.00e-01f  2\n",
      "   6  1.4822950e+04 0.00e+00 3.13e+01  -4.1 1.35e-02    -  1.00e+00 1.00e+00f  1\n",
      "   7  1.4822252e+04 0.00e+00 3.13e+01  -2.7 1.56e-01    -  1.00e+00 2.50e-01f  3\n",
      "   8  1.4820205e+04 0.00e+00 2.87e+01  -3.3 1.09e-01    -  1.00e+00 1.00e+00f  1\n",
      "   9  1.4819554e+04 0.00e+00 6.38e+00  -9.4 3.98e-02    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  1.4819510e+04 0.00e+00 4.54e+00  -6.3 2.82e-02    -  1.00e+00 2.50e-01f  3\n",
      "  11  1.4819451e+04 0.00e+00 3.28e+00  -6.4 7.49e-03    -  1.00e+00 1.00e+00f  1\n",
      "  12  1.4819446e+04 0.00e+00 3.09e+00  -7.2 1.77e-02    -  1.00e+00 1.25e-01f  4\n",
      "  13  1.4819436e+04 0.00e+00 1.80e+00  -8.4 7.05e-03    -  1.00e+00 1.00e+00f  1\n",
      "  14  1.4819432e+04 0.00e+00 9.02e-01 -10.2 1.41e-03    -  1.00e+00 1.00e+00f  1\n",
      "  15  1.4819431e+04 0.00e+00 2.07e-02 -11.0 9.81e-04    -  1.00e+00 1.00e+00f  1\n",
      "  16  1.4819431e+04 0.00e+00 3.51e-03 -11.0 9.03e-06    -  1.00e+00 1.00e+00f  1\n",
      "  17  1.4819431e+04 0.00e+00 1.49e-03 -11.0 1.04e-05    -  1.00e+00 1.00e+00f  1\n",
      "  18  1.4819431e+04 0.00e+00 6.84e-04 -11.0 2.59e-06    -  1.00e+00 2.50e-01f  3\n",
      "  19  1.4819431e+04 0.00e+00 1.73e-04 -11.0 1.98e-06    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20  1.4819431e+04 0.00e+00 2.08e-04 -11.0 1.95e-07    -  1.00e+00 5.00e-01f  2\n",
      "  21  1.4819431e+04 0.00e+00 1.53e-05 -11.0 1.42e-07    -  1.00e+00 1.00e+00f  1\n",
      "  22  1.4819431e+04 0.00e+00 1.89e-05 -11.0 4.12e-08    -  1.00e+00 1.00e+00f  1\n",
      "  23  1.4819431e+04 0.00e+00 8.67e-06 -11.0 1.19e-08    -  1.00e+00 5.00e-01f  2\n",
      "  24  1.4819431e+04 0.00e+00 6.09e-07 -11.0 1.24e-08    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 24\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   6.6146373414677873e+03    1.4819431052626263e+04\n",
      "Dual infeasibility......:   6.0876453413178413e-07    1.3638758370460025e-06\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   9.9999999999999897e-12    2.2403996300329019e-11\n",
      "Overall NLP error.......:   6.0876453413178413e-07    1.3638758370460025e-06\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 70\n",
      "Number of objective gradient evaluations             = 25\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      1.920\n",
      "Total CPU secs in NLP function evaluations           =      0.419\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      " 16.615468 seconds (51.71 M allocations: 2.595 GiB, 2.32% gc time, 96.64% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time optm = QuasiCopula.fit!(qc_model,\n",
    "    Ipopt.IpoptSolver(\n",
    "        print_level = 5, \n",
    "        tol = 10^-6, \n",
    "        max_iter = 100,\n",
    "        accept_after_max_steps = 4,\n",
    "        warm_start_init_point=\"yes\", \n",
    "        limited_memory_max_history = 6, # default value\n",
    "        hessian_approximation = \"limited-memory\",\n",
    "#         derivative_test=\"second-order\"\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βtrue = [1.0, 0.5, 0.5]\n",
      "qc_model.β = [1.0091896288240927, 0.49068185517601387, 0.5016364218552014]\n",
      "qc_model.∇β = [3.3382613090493507e-7, -8.106559831189131e-7, -1.3638758370460025e-6]\n",
      "θtrue = [0.1, 0.1]\n",
      "qc_model.θ = [0.10529641840791103, 0.11976101213786954]\n",
      "qc_model.∇θ = [-6.708739634930794e-7, -1.8532789480829592e-7]\n"
     ]
    }
   ],
   "source": [
    "@show βtrue\n",
    "@show qc_model.β\n",
    "@show qc_model.∇β\n",
    "\n",
    "@show θtrue\n",
    "@show qc_model.θ\n",
    "@show qc_model.∇θ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is $\\nabla_\\beta res$ calculated correctly? \n",
    "\n",
    "We can check using ForwardDiff\n",
    "\n",
    "The function is \n",
    "\n",
    "$$res_{ij}(\\beta) = \\frac{y_{ij} - \\mu_{ij}}{\\sqrt{\\sigma_{ij}^2(\\beta)}}$$\n",
    "\n",
    "### Normal\n",
    "\n",
    "Assumes y, X are given. We calculate the residuals for just 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [0.1296459602031359, -1.5158332856428878, 0.19631145529958616, -0.03059313160267374, -0.8880321856615292]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       "  2.07458     2.07458\n",
       " -1.94686    -1.94686\n",
       "  0.0808759   0.0808759\n",
       "  0.154606    0.154606\n",
       " -0.931964   -0.931964\n",
       " -2.26098    -2.26098\n",
       " -1.19819    -1.19819\n",
       "  0.0763038   0.0763038\n",
       " -0.470584   -0.470584\n",
       " -1.03039    -1.03039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(IdentityLink(), η)\n",
    "    varμ = GLM.glmvar.(Normal(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    for i in 1:p, j in 1:d\n",
    "        ∇resβ[j, i] = -X[j, i]\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(qc_model.β) = [0.5344439504497369, 0.02670454905299698, 0.7098215386604426, 0.44787592964791534, 0.08710902598335912]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.267222   -0.267222\n",
       " -0.0133523  -0.0133523\n",
       " -0.354911   -0.354911\n",
       " -0.223938   -0.223938\n",
       " -0.0435545  -0.0435545\n",
       "  0.554373    0.554373\n",
       " -0.025995   -0.025995\n",
       "  0.0287037   0.0287037\n",
       "  0.0346223   0.0346223\n",
       " -0.0405912  -0.0405912\n",
       " -0.604183   -0.604183\n",
       " -0.0159986  -0.0159986\n",
       "  0.0270811   0.0270811\n",
       " -0.105382   -0.105382\n",
       " -0.0448782  -0.0448782"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β::AbstractVector{T}) where T\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - \n",
    "            (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# check objective\n",
    "@show resβ(qc_model.β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [-0.20116605438121704, 654.5724663515489, -0.04513275044062135, 3.5718353780954617, 70.68722761611653]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       "   -1.73497      -1.73497\n",
       " -329.553      -329.553\n",
       "   -1.00025      -1.00025\n",
       "   -3.03142      -3.03142\n",
       "  -37.1641      -37.1641\n",
       "    3.59933       3.59933\n",
       " -641.594      -641.594\n",
       "    0.0808965     0.0808965\n",
       "    0.468677      0.468677\n",
       "  -34.6356      -34.6356\n",
       "   -3.92273      -3.92273\n",
       " -394.867      -394.867\n",
       "    0.0763233     0.0763233\n",
       "   -1.42654      -1.42654\n",
       "  -38.2936      -38.2936"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Poisson(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(LogLink(), η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        ∇resβ[j, i] = x_ji * (-(inv(sqrt(varμ_j)) + (0.5 * inv(varμ_j)) * res_j) * dμ_j)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autodiff_loglikelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function A_mul_b!(c::AbstractVector{T}, A::AbstractMatrix, b::AbstractVector) where T\n",
    "    n, p = size(A)\n",
    "    fill!(c, zero(T))\n",
    "    for j in 1:p, i in 1:n\n",
    "        c[i] += A[i, j] * b[j]\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "    ) where T\n",
    "    θ = qc_model.θ\n",
    "    # allocate vector of type T\n",
    "    n, p = size(qc_model.data[1].X)\n",
    "    η = zeros(T, n)\n",
    "    μ = zeros(T, n)\n",
    "    varμ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for (i, gc) in enumerate(qc_model.data)\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(η, X, β)\n",
    "        for i in 1:gc.n\n",
    "            μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "            varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             w2[i] = w1[i] * dμ[i]\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        for j in eachindex(y)\n",
    "            res[j] /= sqrt(varμ[j])\n",
    "        end\n",
    "        # std_res_differential! step (this will compute ∇resβ)\n",
    "#         for i in 1:gc.p\n",
    "#             for j in 1:gc.n\n",
    "#                 ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "#             end\n",
    "#         end\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        # component_loglikelihood\n",
    "        for j in 1:gc.n\n",
    "            logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], 1.0, 1.0)\n",
    "        end\n",
    "        tsum = dot(θ, gc.t)\n",
    "        logl += -log(1 + tsum)\n",
    "        qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    gcm::GaussianCopulaVCModel\n",
    "    ) where T\n",
    "    θ = gcm.θ\n",
    "    τ = gcm.τ[1]\n",
    "    # allocate vector of type T\n",
    "    n, p = size(gcm.data[1].X)\n",
    "    μ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for gc in gcm.data\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        sqrtτ = sqrt(abs(τ))\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(μ, X, β)\n",
    "        for i in 1:gc.n\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        res .*= sqrtτ\n",
    "        rss  = abs2(norm(res)) # RSS of standardized residual\n",
    "        tsum = dot(θ, gc.t) # ben: why is there abs here?\n",
    "        logl += - log(1 + tsum) - (gc.n * log(2π) -  gc.n * log(abs(τ)) + rss) / 2\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        qsum  = dot(θ, q)\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "# sample data\n",
    "autodiff_loglikelihood(β) = loglikelihood(β, qc_model)\n",
    "\n",
    "# function grad_loglikelihood(\n",
    "#     β::AbstractVector{T}, \n",
    "#     qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "#     ) where T\n",
    "#     β = qc_model.β\n",
    "#     n, p = size(qc_model.data[1].X)\n",
    "#     η = zeros(T, n)\n",
    "#     μ = zeros(T, n)\n",
    "#     varμ = zeros(T, n)\n",
    "#     res = zeros(T, n)\n",
    "#     w1 = zeros(T, n)\n",
    "#     dμ = zeros(T, n)\n",
    "#     storage_n = zeros(T, n)\n",
    "#     out = zeros(length(β))\n",
    "#     for gc in qc_model.data\n",
    "#         X = gc.X\n",
    "#         y = gc.y\n",
    "#         n, p = size(X)\n",
    "#         # update_res! step\n",
    "#         A_mul_b!(η, X, β)\n",
    "#         for i in 1:gc.n\n",
    "#             μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "#             varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             res[i] = y[i] - μ[i]\n",
    "#         end\n",
    "#         # GLM gradient\n",
    "#         out += X' * Diagonal(w1) * res\n",
    "#         # 2nd gradient term\n",
    "#         res ./= sqrt.(varμ)\n",
    "        \n",
    "#     end\n",
    "#     return out\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8635.392759793393, -8635.39275979312)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autodiff_loglikelihood(qc_model.β), loglikelihood!(qc_model, true, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta is $\\pm 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 9.659198719758066e-6\n",
       " 2.194933313059977e-5\n",
       " 8.53400730504994e-6"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 3.149173630434543e-5\n",
       " 7.270955512718447e-5\n",
       " 6.383652480647373e-5"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta is $\\pm 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.2863324753430447e-6\n",
       " -7.904796714974793e-7\n",
       "  2.0348114821988617e-6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.286331556300425e-6\n",
       " -7.904736671893176e-7\n",
       "  2.0348091102906363e-6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.2863324753430447e-6\n",
       " -7.904796714974793e-7\n",
       "  2.0348114821988617e-6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.286331556300425e-6\n",
       " -7.904736671893176e-7\n",
       "  2.0348091102906363e-6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my naive gradient (does not work)\n",
    "# function compute_∇resβ(β, X, y, dist, link)\n",
    "#     d, p = size(X)\n",
    "#     ∇resβ = zeros(d, p)\n",
    "#     η = X * β # d by 1\n",
    "#     μ = GLM.linkinv.(link, η) # d by 1\n",
    "#     varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "#     res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "#     for i in 1:p, j in 1:d\n",
    "#         varμ_j = varμ[j]\n",
    "#         x_ji = X[j, i]\n",
    "#         res_j = res[j]\n",
    "#         μ_j = μ[j]\n",
    "#         ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - \n",
    "#             (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "#     end\n",
    "#     return ∇resβ # d × p\n",
    "# end\n",
    "# function grad_logl_sample_i(dist, link, Γ, X, y, β)\n",
    "#     η = X*β\n",
    "#     μ = GLM.linkinv.(link, η)\n",
    "#     varμ = GLM.glmvar.(dist, μ)\n",
    "#     res = (y .- μ) ./ sqrt.(varμ)\n",
    "#     denom = 1 + 0.5 * (res' * Γ * res)\n",
    "#     ∇resβ = compute_∇resβ(β, X, y, dist, link)\n",
    "#     W1 = GLM.mueta.(link, η) / GLM.glmvar.(dist, μ)\n",
    "#     return X' * Diagonal(W1) * (y - μ) + ∇resβ'*Γ*res / denom\n",
    "# end\n",
    "\n",
    "# ∇β_test = zeros(3)\n",
    "# for i in 1:length(qc_model.data)\n",
    "#     Γ = sum(qc_model.θ .* qc_model.data[i].V)\n",
    "#     X = qc_model.data[i].X\n",
    "#     y = qc_model.data[i].y\n",
    "#     ∇β_test += grad_logl_sample_i(Bernoulli(), LogitLink(), Γ, X, y, qc_model.β)\n",
    "# end\n",
    "# ∇β_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  8.57327120407092e-8\n",
       "  2.7007899694453386e-8\n",
       " -1.1578850189764012e-7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  8.528801986873447e-8\n",
       "  2.6077807413482645e-8\n",
       " -1.1694493039574039e-7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta^2 L$\n",
    "\n",
    "Hessians for a single observation seems to differ quite a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function two_term_Hessian(gcm::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function two_term_Hessian(gcm::GaussianCopulaVCModel)\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = -sqrt(gcm.τ[1]) .* gc.X # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function three_term_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "#     # sarah's implementation\n",
    "#     loglikelihood!(qc_model, true, true)\n",
    "#     return qc_model.Hβ\n",
    "#     @show qc_model.Hβ\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in qc_model.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "# autodiff ∇²resβ (giving some kind of tensor)\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# this is d²rᵢₖ(β) needed for computing the 4th hessian term\n",
    "function r_ik(β, k)\n",
    "    res = resβ(β)\n",
    "    return res[k]\n",
    "end\n",
    "r_ik(β) = r_ik(β, k)\n",
    "∇²r_ik = x -> ForwardDiff.hessian(r_ik, x)\n",
    "\n",
    "function full_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)    \n",
    "    # loop over samples\n",
    "    for (i, gc) in enumerate(qc_model.data)\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "        # 4th term\n",
    "        ek = zeros(d)    \n",
    "        for k in 1:d\n",
    "            # somehow need to define autodiff functions here, or else k is treated as fixed\n",
    "            function resβ(β)\n",
    "                η = X * β # d by 1\n",
    "                μ = GLM.linkinv.(LogitLink(), η)\n",
    "                varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "                return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "            end\n",
    "            r_ik(β, k) = resβ(β)[k]\n",
    "            r_ik(β) = r_ik(β, k)\n",
    "            ∇²r_ik = x -> ForwardDiff.hessian(r_ik, x) \n",
    "            \n",
    "            \n",
    "            fill!(ek, 0)\n",
    "            ek[k] = 1\n",
    "            X = gc.X\n",
    "            y = gc.y\n",
    "#             @show ∇²r_ik(qc_model.β)\n",
    "            H += (ek' * Γ * res * ∇²r_ik(qc_model.β)) / denom\n",
    "        end\n",
    "    end\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×3 Matrix{Float64}:\n",
       " -0.253406    0.525711    -0.572947\n",
       " -0.279741   -0.544617    -0.335183\n",
       " -0.263081    0.0212769    0.0200741\n",
       " -0.263562    0.0407485   -0.124028\n",
       " -0.272238   -0.253716    -0.280512\n",
       "  0.525711   -1.09063      1.18862\n",
       " -0.544617   -1.06029     -0.652555\n",
       "  0.0212769  -0.00172079  -0.00162351\n",
       "  0.0407485  -0.00629998   0.0191756\n",
       " -0.253716   -0.236454    -0.261427\n",
       " -0.572947    1.18862     -1.29542\n",
       " -0.335183   -0.652555    -0.401613\n",
       "  0.0200741  -0.00162351  -0.00153173\n",
       " -0.124028    0.0191756   -0.0583658\n",
       " -0.280512   -0.261427    -0.289037"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directly evaluating ∇²resβ (giving some kind of tensor)\n",
    "Hββ = ∇²resβ_autodiff(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.253406   0.525711  -0.572947\n",
       "  0.525711  -1.09063    1.18862\n",
       " -0.572947   1.18862   -1.29542"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 1\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.279741  -0.544617  -0.335183\n",
       " -0.544617  -1.06029   -0.652555\n",
       " -0.335183  -0.652555  -0.401613"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 2\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -5800.35       28.5649     84.1043\n",
       "    28.5649  -5155.84       59.5103\n",
       "    84.1043     59.5103  -5164.75"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term Hessian from math\n",
    "two_terms_H = two_term_Hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian from math\n",
    "three_term_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian implemented by sarah\n",
    "loglikelihood!(qc_model, true, true)\n",
    "qc_model.Hβ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 term Hessian from math\n",
    "full_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(\n",
    "        autodiff_loglikelihood, x)\n",
    "autodiff_H = ∇²logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check $\\nabla_{\\theta}L$, $\\nabla^2_{\\theta}L$, and $\\nabla_{\\theta}\\nabla_{\\beta} L$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#22 (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loglikelihood function friendly to autodiff\n",
    "autodiff_loglikelihood(β) = QuasiCopula.loglikelihood(β, qc_model, z)\n",
    "\n",
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "\n",
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(autodiff_loglikelihood, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if `autodiff_loglikelihood` returns same answer as `QuasiCopula.loglikelihood!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff_loglikelihood(fullβ) = -18033.163626812184\n",
      "QuasiCopula.loglikelihood!(qc_model, false, false) = -18033.16362681217\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "# fullβ = [qc_model.β; qc_model.θ; qc_model.τ; 0.0] # normal\n",
    "\n",
    "@show autodiff_loglikelihood(fullβ)\n",
    "@show QuasiCopula.loglikelihood!(qc_model, false, false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.1370986533992892e-6\n",
       "  2.3139215938341664e-6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇logl(fullβ)[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.137098653399271e-6\n",
       "  2.3139215938342303e-6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "grad_math = zeros(m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    grad_math += b / (1 + qc_model.θ'*b) - c / (1 + qc_model.θ'*c)\n",
    "end\n",
    "grad_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta^2 L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 1.25985e-15   1.0245e-15\n",
       " 1.0245e-15   -9.38656e-15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[4:5, 4:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " -1.25985e-15  -1.0245e-15\n",
       " -1.0245e-15    9.38656e-15"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "hess_math = zeros(m, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    hess_math += b*b' / (1 + qc_model.θ'*b)^2 - c*c' / (1 + qc_model.θ'*c)^2\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[1:3, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "p = size(qc_model.data[i].X, 2)\n",
    "hess_math = zeros(p, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    θ = qc_model.θ\n",
    "    ∇resβ = qc_model.data[i].∇resβ\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    A = hcat([∇resβ' * Ω[k] * r for k in 1:m]...)\n",
    "    hess_math += A ./ (1 + θ'*b) - (A*θ ./ (1 + θ'*b)^2) * b'\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check $\\frac{\\partial^2\\mu}{\\partial \\eta^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mueta2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    mueta2(l::Link, η::Real)\n",
    "\n",
    "Second derivative of the inverse link function `d^2μ/dη^2`, for link `L` at linear predictor value `η`.\n",
    "I.e. derivative of the mueta function in GLM.jl\n",
    "\"\"\"\n",
    "function mueta2 end\n",
    "\n",
    "mueta2(::IdentityLink, η::Real) = zero(η)\n",
    "function mueta2(::LogitLink, η::Real)\n",
    "    expabs = exp(-abs(η))\n",
    "    denom = 1 + expabs\n",
    "    return -expabs / denom^2 + 2expabs^2 / denom^3\n",
    "end\n",
    "mueta2(::LogLink, η::Real) = exp(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mueta2(l, η) = -0.015346957645411913\n",
      "mueta2_autodiff(η) = -0.015346957645411843\n"
     ]
    }
   ],
   "source": [
    "# test mueta2 function\n",
    "# l = IdentityLink()\n",
    "# l = LogLink()\n",
    "l = LogitLink()\n",
    "η = 0.1234\n",
    "μ = GLM.linkinv(l, η)\n",
    "\n",
    "# mathematical hessian\n",
    "@show mueta2(l, η)\n",
    "\n",
    "# ForwardDiff Hessian\n",
    "logit_mueta = η -> GLM.mueta(l, η)\n",
    "mueta2_autodiff = x -> ForwardDiff.derivative(logit_mueta, x)\n",
    "@show mueta2_autodiff(η);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial\\sigma^2}{\\partial \\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmamu (generic function with 3 methods)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaeta(D::Distribution, μ::Real)\n",
    "\n",
    "Computes dσ²/dμ\n",
    "\"\"\"\n",
    "function sigmamu end\n",
    "\n",
    "sigmamu(::Normal, μ::Real) = zero(μ)\n",
    "sigmamu(::Bernoulli, μ::Real) = one(μ) - 2μ\n",
    "sigmamu(::Poisson, μ::Real) = one(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial^2(\\sigma^2)}{\\partial \\mu^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmamu2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaμ2(D::Distribution, μ::Real)\n",
    "\n",
    "Computes d²σ²/dμ²\n",
    "\"\"\"\n",
    "function sigmamu2 end\n",
    "\n",
    "sigmamu2(::Normal, μ::Real) = zero(μ)\n",
    "sigmamu2(::Bernoulli, μ::Real) = -2\n",
    "sigmamu2(::Poisson, μ::Real) = zero(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla \\mu$, $\\nabla^2 \\mu$, $\\nabla \\sigma^2$ and $\\nabla^2 \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_μj(X, β, link, j) = 0.5614464525554457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×2 Matrix{Float64}:\n",
       " -0.0302592   -0.0302592\n",
       "  0.033953     0.033953\n",
       "  0.0127624    0.0127624\n",
       "  0.033953     0.033953\n",
       " -0.0380978   -0.0380978\n",
       " -0.0143203   -0.0143203\n",
       "  0.0127624    0.0127624\n",
       " -0.0143203   -0.0143203\n",
       " -0.00538278  -0.00538278"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ∇²μ_j(l::Link, Xi::Matrix, β::Vector, j)\n",
    "\n",
    "Computes the Hessian of the mean function with respect to β for sample i (Xi) at time j\n",
    "\"\"\"\n",
    "function ∇²μ_j(l::Link, Xi::Matrix, β::Vector, j)\n",
    "    xj = Xi[j, :]\n",
    "    ηj = dot(xj, β)\n",
    "    d²μdη² = mueta2(l, ηj)\n",
    "    return d²μdη² * xj * xj'\n",
    "end\n",
    "\n",
    "# objective \n",
    "function eval_μj(X, β, link, j)\n",
    "    η = X*β\n",
    "    μj = GLM.linkinv(link, η[j])\n",
    "    return μj\n",
    "end\n",
    "\n",
    "# autodiff hessian\n",
    "eval_μj(β) = eval_μj(X, β, link, j)\n",
    "∇²μ_j_autodiff = x -> ForwardDiff.hessian(eval_μj, x)\n",
    "\n",
    "# data\n",
    "link = LogitLink()\n",
    "X = qc_model.data[1].X\n",
    "β = qc_model.β\n",
    "j = 1\n",
    "@show eval_μj(X, β, link, j)\n",
    "\n",
    "# compare autodiff and mathematical result\n",
    "math_result = ∇²μ_j(link, X, β, j)\n",
    "autodiff_result = ∇²μ_j_autodiff(β)\n",
    "\n",
    "[vec(math_result) vec(autodiff_result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_σ2j(X, β, dist, link, j) = 0.24622433346835138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×2 Matrix{Float64}:\n",
       " -0.117534   -0.117534\n",
       "  0.131882    0.131882\n",
       "  0.0495722   0.0495722\n",
       "  0.131882    0.131882\n",
       " -0.147981   -0.147981\n",
       " -0.0556237  -0.0556237\n",
       "  0.0495722   0.0495722\n",
       " -0.0556237  -0.0556237\n",
       " -0.020908   -0.020908"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ∇²σ²_j(d::Distribution, l::Link, Xi::Matrix, β::Vector, j)\n",
    "\n",
    "Computes the Hessian of the σ^2 function with respect to β for sample i (Xi) at time j\n",
    "\"\"\"\n",
    "function ∇²σ²_j(d::Distribution, l::Link, Xi::Matrix, β::Vector, j)\n",
    "    xj = Xi[j, :]\n",
    "    ηj = dot(xj, β)\n",
    "    μj = GLM.linkinv.(l, ηj)\n",
    "    c = sigmamu2(d, μj)*GLM.mueta(l, ηj)^2 + sigmamu(d, μj)*mueta2(l, ηj)\n",
    "    return c * xj * xj'\n",
    "end\n",
    "\n",
    "# objective\n",
    "function eval_σ2j(X, β, dist, link, j)\n",
    "    η = X*β\n",
    "    μ = GLM.linkinv.(link, η)\n",
    "    σ2j = GLM.glmvar(dist, μ[j])\n",
    "    return σ2j\n",
    "end\n",
    "\n",
    "# autodiff hessian\n",
    "eval_σ2j(β) = eval_σ2j(X, β, dist, link, j)\n",
    "∇²σ2j_autodiff = x -> ForwardDiff.hessian(eval_σ2j, x)\n",
    "\n",
    "# data\n",
    "dist = Bernoulli()\n",
    "link = LogitLink()\n",
    "X = qc_model.data[1].X\n",
    "β = qc_model.β\n",
    "j = 1\n",
    "@show eval_σ2j(X, β, dist, link, j)\n",
    "\n",
    "# compare autodiff and mathematical result\n",
    "math_result = ∇²σ²_j(dist, link, X, β, j)\n",
    "autodiff_result = ∇²σ2j_autodiff(β)\n",
    "\n",
    "[vec(math_result) vec(autodiff_result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that we can compute $∇resβ$ generally, although we don't do so in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.565735   -0.565735\n",
       " -0.449521   -0.449521\n",
       " -0.736097   -0.736097\n",
       " -0.23579    -0.23579\n",
       " -0.27525    -0.27525\n",
       "  0.634795    0.634795\n",
       "  0.496558    0.496558\n",
       "  0.306947    0.306947\n",
       " -0.0678103  -0.0678103\n",
       " -0.0632576  -0.0632576\n",
       "  0.238609    0.238609\n",
       "  0.609367    0.609367\n",
       "  0.045592    0.045592\n",
       " -0.165939   -0.165939\n",
       " -0.0394505  -0.0394505"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(X, y, β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "resβ(β) = resβ(X, y, β)\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(X, y, β::AbstractVector{T}) where T\n",
    "    dist = Bernoulli()\n",
    "    link = LogitLink()\n",
    "    \n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(link, η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        dμdβ = dμ_j * x_ji\n",
    "        dσ2dβ = sigmamu(dist, μ_j) * dμdβ\n",
    "        # in practice, we have update_∇resβ fucntions to compute ∇resβ[j, i]\n",
    "        ∇resβ[j, i] = -inv(sqrt(varμ_j)) * dμdβ - 0.5 * res_j * inv(varμ_j) * dσ2dβ\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇resβ(β) = ∇resβ(X, y, β)\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $∇^2resβ$: Hessian of residual vector of sample $i$ at observation $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.282867   0.317398   0.119305\n",
       "  0.317398  -0.356143  -0.133868\n",
       "  0.119305  -0.133868  -0.0503189"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical Hessian of residuals wrt β for sample i at time j\n",
    "# note: need function sigmamu, sigmamu2, mueta2\n",
    "function ∇²resβ_ij(qc_model, i, j)\n",
    "    dist = Bernoulli()\n",
    "    link = LogitLink()\n",
    "    \n",
    "    X = qc_model.data[i].X\n",
    "    y = qc_model.data[i].y\n",
    "    β = qc_model.β\n",
    "    xj = X[j, :]\n",
    "    d, p = size(X)\n",
    "    \n",
    "    # intermediate quantities?\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    invσ = inv.(sqrt.(varμ))\n",
    "    ∇μ_ij  = GLM.mueta(link, η[j]) * xj\n",
    "    ∇σ²_ij = sigmamu(dist, μ[j]) * GLM.mueta(link, η[j]) * xj\n",
    "\n",
    "    # assemble 5 terms\n",
    "    term1 = -invσ[j] * ∇²μ_j(link, X, β, j)\n",
    "    term2 = 0.5invσ[j]^3 * ∇σ²_ij * ∇μ_ij'\n",
    "    term3 = -0.5 * res[j] * inv(varμ[j]) * ∇²σ²_j(dist, link, X, β, j)\n",
    "    term4 = 0.5invσ[j]^3 * ∇μ_ij * ∇σ²_ij'\n",
    "    term5 = 0.75res[j] * inv(varμ[j]^2) * ∇σ²_ij * ∇σ²_ij'\n",
    "    ∇²resβ_ik = term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "    return ∇²resβ_ik # p × p\n",
    "end\n",
    "i = 1 # sample id\n",
    "j = 1 # time point\n",
    "∇²resβ_ij(qc_model, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.282867   0.317398   0.119305\n",
       "  0.317398  -0.356143  -0.133868\n",
       "  0.119305  -0.133868  -0.0503189"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian of residuals wrt β for sample i at time j\n",
    "β = qc_model.β\n",
    "# gc = qc_model.data[1]\n",
    "T = eltype(β)\n",
    "X = qc_model.data[i].X\n",
    "y = qc_model.data[i].y\n",
    "\n",
    "p = length(β)\n",
    "d = qc_model.data[i].n\n",
    "H = zeros(T, p, p)\n",
    "ek = zeros(T, d)\n",
    "k = 1\n",
    "\n",
    "function resβ(X, y, β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "r_ik(β, k) = resβ(X, y, β)[k]\n",
    "r_ik(β) = r_ik(β, k)\n",
    "∇²r_ik = x -> ForwardDiff.hessian(r_ik, x) \n",
    "∇²r_ik(β)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $d_\\gamma d_\\gamma  r_{ik}$: Hessian of residual vector of sample $i$ at observation $j$\n",
    "\n",
    "Because $\\gamma$ is a scalar, the resulting Hessian is a vector with length of $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09308160081535394"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # math (note: this is same as dβdβ_res_ij but we just plug in z in place of xj)\n",
    "# function dγdγ_res_ij(dist, link, xj, η_j, μ_j, varμ_j, res_j)\n",
    "#     invσ_j = inv(sqrt(varμ_j))\n",
    "#     ∇μ_ij  = GLM.mueta(link, η_j) * xj\n",
    "#     ∇σ²_ij = sigmamu(dist, μ_j) * GLM.mueta(link, η_j) * xj\n",
    "\n",
    "#     # assemble 5 terms\n",
    "#     term1 = -invσ_j * QuasiCopula.∇²μ_j(link, η_j, xj)\n",
    "#     term2 = 0.5invσ_j^3 * ∇σ²_ij * ∇μ_ij'\n",
    "#     term3 = -0.5 * res_j * inv(varμ_j) * QuasiCopula.∇²σ²_j(dist, link, xj, μ_j, η_j)\n",
    "#     term4 = 0.5invσ_j^3 * ∇μ_ij * ∇σ²_ij'\n",
    "#     term5 = 0.75res_j * inv(varμ_j^2) * ∇σ²_ij * ∇σ²_ij'\n",
    "#     result = term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "#     return result # 1 × 1\n",
    "# end\n",
    "# z = rand()\n",
    "# η_j = rand()\n",
    "# μ_j = rand()\n",
    "# varμ_j = rand()\n",
    "# res_j = rand()\n",
    "# dγdγ_res_ij(Bernoulli(), LogitLink(), z, η_j, μ_j, varμ_j, res_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::Vector{Float64}, ::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}})\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/operators.jl:655\n\u001b[0m  *(\u001b[91m::StridedMatrix{T}\u001b[39m, ::StridedVector{S}) where {T<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:Real} at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/stdlib/v1.7/LinearAlgebra/src/matmul.jl:44\n\u001b[0m  *(::StridedVecOrMat, \u001b[91m::Adjoint{<:Any, <:LinearAlgebra.LQPackedQ}\u001b[39m) at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/stdlib/v1.7/LinearAlgebra/src/lq.jl:266\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::Vector{Float64}, ::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}})\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/base/operators.jl:655\n\u001b[0m  *(\u001b[91m::StridedMatrix{T}\u001b[39m, ::StridedVector{S}) where {T<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:Real} at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/stdlib/v1.7/LinearAlgebra/src/matmul.jl:44\n\u001b[0m  *(::StridedVecOrMat, \u001b[91m::Adjoint{<:Any, <:LinearAlgebra.LQPackedQ}\u001b[39m) at /Applications/Julia-1.7.app/Contents/Resources/julia/share/julia/stdlib/v1.7/LinearAlgebra/src/lq.jl:266\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] r_j(y::Vector{Float64}, X::Matrix{Float64}, z::Vector{Float64}, β::Vector{Float64}, γ::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}}, j::Int64)",
      "    @ Main ./In[139]:3",
      "  [2] r_j(γ::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}})",
      "    @ Main ./In[139]:9",
      "  [3] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      "  [4] vector_mode_gradient(f::typeof(r_j), x::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:106",
      "  [5] gradient",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:19 [inlined]",
      "  [6] #114",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:16 [inlined]",
      "  [7] vector_mode_dual_eval!",
      "    @ ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37 [inlined]",
      "  [8] vector_mode_jacobian(f::ForwardDiff.var\"#114#115\"{typeof(r_j), ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}}}, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:148",
      "  [9] jacobian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}}, ::Val{false})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/jacobian.jl:21",
      " [10] hessian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}}, ::Val{true})",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:17",
      " [11] hessian(f::Function, x::Vector{Float64}, cfg::ForwardDiff.HessianConfig{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(r_j), Float64}, Float64, 1}}}) (repeats 2 times)",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/hessian.jl:15",
      " [12] (::var\"#157#158\")(x::Vector{Float64})",
      "    @ Main ./In[139]:10",
      " [13] top-level scope",
      "    @ In[139]:15",
      " [14] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [15] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# # autodiff\n",
    "# function r_j(y, X, z, β, γ, j)\n",
    "#     η = X * β + z * γ\n",
    "#     μ = GLM.linkinv.(LogitLink(), η)\n",
    "#     varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "#     res = (y - μ) ./ sqrt.(varμ)\n",
    "#     return res[j]\n",
    "# end\n",
    "# r_j(γ) = r_j(y, X, z, β, γ, j)\n",
    "# auto_dγdγ_res_ij = x -> ForwardDiff.hessian(r_j, x) \n",
    "\n",
    "# γ = rand()\n",
    "# z = rand(5)\n",
    "# r_j(γ)\n",
    "# auto_dγdγ_res_ij([γ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check Q term\n",
    "\n",
    "When $\\beta$ have large effects, the Hessian with respect to beta no longer match autodiff. Lets try to reproduce this behavior\n",
    "\n",
    "+ $Q = d_{\\gamma}^2 \\mathcal{L}$ is the Hessian of the loglikelihood with respect to the effect of the SNP $\\gamma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loglikelihood_i (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function loglikelihood_i(\n",
    "    par::AbstractVector{T}, # p+m+1 × 1 where m is number of VCs, 1 is for the SNP\n",
    "    qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}, # fitted null model\n",
    "    z::AbstractVector, # n × 1 genotype vector\n",
    "    i::Int # sample log\n",
    "    ) where T\n",
    "    p = qc_model.p\n",
    "    m = length(par) - 1 - p\n",
    "    β = [par[1:end-(m+1)]; par[end]] # nongenetic + genetic beta\n",
    "    θ = par[end-m:end-1]             # vc parameters\n",
    "    # allocate storage vectors of type T\n",
    "    nmax = maximum(size(qc_model.data[i].X, 1) for i in 1:length(qc_model.data))\n",
    "    η_store = zeros(T, nmax)\n",
    "    μ_store = zeros(T, nmax)\n",
    "    varμ_store = zeros(T, nmax)\n",
    "    res_store = zeros(T, nmax)\n",
    "    storage_n_store = zeros(T, nmax)\n",
    "    Xstore = zeros(T, nmax, p+1)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "        \n",
    "    gc = qc_model.data[i]\n",
    "        n = size(gc.X, 1)\n",
    "        X = @view(Xstore[1:n, :])\n",
    "        η = @view(η_store[1:n])\n",
    "        μ = @view(μ_store[1:n])\n",
    "        varμ = @view(varμ_store[1:n])\n",
    "        res = @view(res_store[1:n])\n",
    "        storage_n = @view(storage_n_store[1:n])\n",
    "        # sync nogenetic + genetic covariates\n",
    "        copyto!(X, gc.X)\n",
    "        X[:, end] .= z[i]\n",
    "        y = gc.y\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        QuasiCopula.A_mul_b!(η, X, β)\n",
    "        for j in 1:gc.n\n",
    "            μ[j] = GLM.linkinv(gc.link, η[j])\n",
    "            varμ[j] = GLM.glmvar(gc.d, μ[j]) # Note: for negative binomial, d.r is used\n",
    "            # dμ[j] = GLM.mueta(gc.link, η[j])\n",
    "            # w1[j] = dμ[j] / varμ[j]\n",
    "            # w2[j] = w1[j] * dμ[j]\n",
    "            res[j] = y[j] - μ[j]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        for j in eachindex(y)\n",
    "            res[j] /= sqrt(varμ[j])\n",
    "        end\n",
    "        # std_res_differential! step (this will compute ∇resβ)\n",
    "        # for i in 1:gc.p\n",
    "        #     for j in 1:gc.n\n",
    "        #         ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "        #     end\n",
    "        # end\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            QuasiCopula.A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        # component_loglikelihood\n",
    "        for j in 1:gc.n\n",
    "            logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], one(T), one(T))\n",
    "        end\n",
    "        tsum = dot(θ, gc.t)\n",
    "        logl += -log(1 + tsum)\n",
    "        qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "        logl += log(1 + qsum)\n",
    "#     end\n",
    "    return logl\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_Qi (generic function with 3 methods)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculate_Qi(qc_model, gc::GLMCopulaVCObs, z, Γ, i)\n",
    "    β = qc_model.β\n",
    "    X = gc.X\n",
    "    y = gc.y\n",
    "    \n",
    "        dist = gc.d\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # the snp\n",
    "        zi = fill(z[i], gc.n)\n",
    "        # update ∇resγ\n",
    "        ∇resγ = zeros(d)\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        for k in 1:d # loop over each sample's observation\n",
    "            ∇resγ[k] = QuasiCopula.update_∇resβ(dist, zi[k], res[k], gc.μ[k], gc.dμ[k], gc.varμ[k])\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        denom2 = abs2(denom)\n",
    "    \n",
    "    #\n",
    "    # check dβdβ_res_ij for γ term matches autodiff\n",
    "    #\n",
    "    # autodiff\n",
    "    auto_resβ_ij = Float64[]\n",
    "    for k in 1:d\n",
    "        # somehow this has to be inside the loop in order to vary the `k` parameter\n",
    "        function resγ(X, y, β, z, γ)\n",
    "            η = X * β + z .* γ # d by 1: z is vector of size == size(X, 1)\n",
    "            μ = GLM.linkinv.(LogitLink(), η)\n",
    "            varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "            return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "        end\n",
    "        res_ik(γ, k) = resγ(X, y, β, zi, γ)[k] # residual of sample i at measurement k\n",
    "        res_ik(γ) = res_ik(γ, k) \n",
    "        auto_dβdβ_res_ij = x -> ForwardDiff.hessian(res_ik, x) \n",
    "        # evaluate autodiff\n",
    "        push!(auto_resβ_ij, auto_dβdβ_res_ij([0.0])[1])\n",
    "    end\n",
    "    @show auto_resβ_ij\n",
    "    # math \n",
    "    math_resβ_ij = Float64[]\n",
    "    for k in 1:d\n",
    "        push!(math_resβ_ij, QuasiCopula.dβdβ_res_ij(gc.d, gc.link, z[i], gc.η[k], gc.μ[k], gc.varμ[k], res[k]))\n",
    "    end\n",
    "    @show math_resβ_ij\n",
    "    fdsa\n",
    "            \n",
    "    Q = 0.0\n",
    "    Q += Transpose(zi) * Diagonal(gc.w2) * zi\n",
    "    Q += (∇resγ' * Γ * res) * (∇resγ' * Γ * res)' / denom2 # 2nd term\n",
    "    Q -= ∇resγ' * Γ * ∇resγ / denom # 3rd term\n",
    "    ek = zeros(d)\n",
    "    for k in 1:d\n",
    "        fill!(ek, 0)\n",
    "        ek[k] = 1\n",
    "        Q -= (ek' * Γ * res * QuasiCopula.dβdβ_res_ij(gc.d, gc.link, z[i], gc.η[k], gc.μ[k], gc.varμ[k], res[k])) / denom\n",
    "        Q -= numer\n",
    "    end\n",
    "    return Q\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $i = 1$ and $k = 2$, dβdβ_res_ij fails to match autodiff, why??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto = -0.09693671578820441\n",
      "η_j = 0.24703448524051816\n",
      "GLM.mueta(link, η_j) = 0.24622433346835135\n",
      "varμ_j = 0.24622433346835138\n",
      "invσ_j = 2.0152759158475693\n",
      "∇μ_ij = -0.1441397248123729\n",
      "∇σ²_ij = 0.017713749524076932\n",
      "term1 = 0.020897663522326825\n",
      "term2 = -0.010448831761163407\n",
      "term3 = -0.09254470722633086\n",
      "term4 = -0.010448831761163409\n",
      "term5 = -0.00439200856187351\n",
      "math = -0.09693671578820436\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "k = 1\n",
    "gc = qc_model.data[i]\n",
    "y = gc.y\n",
    "X = gc.X\n",
    "z = fill(z[i], size(X, 1))\n",
    "γ = [0.0]\n",
    "\n",
    "# autodiff\n",
    "function resγ(X, y, β, z, γ)\n",
    "    η = X * β + z .* γ # d by 1: z is vector of size == size(X, 1)\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "res_ik(γ, k) = resγ(X, y, β, zi, γ)[k] # residual of sample i at measurement k\n",
    "res_ik(γ) = res_ik(γ, k) \n",
    "auto_dβdβ_res_ij = x -> ForwardDiff.hessian(res_ik, x)\n",
    "auto = auto_dβdβ_res_ij(γ)[1]\n",
    "@show auto\n",
    "\n",
    "# math\n",
    "math = QuasiCopula.dβdβ_res_ij(gc.d, gc.link, z[i], gc.η[k], gc.μ[k], gc.varμ[k], gc.res[k])\n",
    "@show math;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto = -0.07702396925952908\n",
      "η_j = -0.21284889183092182\n",
      "GLM.mueta(link, η_j) = 0.24718970323767067\n",
      "varμ_j = 0.2471897032376707\n",
      "invσ_j = 2.011336856888617\n",
      "∇μ_ij = -0.14470485227533242\n",
      "∇σ²_ij = -0.015342254457215551\n",
      "term1 = 0.01806453186341636\n",
      "term2 = 0.009032265931708191\n",
      "term3 = -0.07788981010525632\n",
      "term4 = 0.009032265931708191\n",
      "term5 = -0.0025975225371817007\n",
      "math = -0.04435826891560527\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "k = 2\n",
    "gc = qc_model.data[i]\n",
    "y = gc.y\n",
    "X = gc.X\n",
    "z = fill(z[i], size(X, 1))\n",
    "γ = [0.0]\n",
    "\n",
    "# autodiff\n",
    "function resγ(X, y, β, z, γ)\n",
    "    η = X * β + z .* γ # d by 1: z is vector of size == size(X, 1)\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "res_ik(γ, k) = resγ(X, y, β, zi, γ)[k] # residual of sample i at measurement k\n",
    "res_ik(γ) = res_ik(γ, k) \n",
    "auto_dβdβ_res_ij = x -> ForwardDiff.hessian(res_ik, x)\n",
    "auto = auto_dβdβ_res_ij(γ)[1]\n",
    "@show auto\n",
    "\n",
    "# math\n",
    "math = QuasiCopula.dβdβ_res_ij(gc.d, gc.link, z[i], gc.η[k], gc.μ[k], gc.varμ[k], gc.res[k])\n",
    "@show math;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_resβ_ij = [-0.09693671578820441, -0.07702396925952908, -0.1261277809575639, 0.04040177775644848, 0.04716318537196372]\n",
      "math_resβ_ij = [-0.09693671578820436, -0.07702396925952905, -0.1261277809575638, 0.04040177775644851, 0.04716318537196369]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: fdsa not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: fdsa not defined",
      "",
      "Stacktrace:",
      " [1] calculate_Qi(qc_model::GLMCopulaVCModel{Float64, Bernoulli{Float64}, LogitLink}, gc::GLMCopulaVCObs{Float64, Bernoulli{Float64}, LogitLink}, z::Vector{Float64}, Γ::Matrix{Float64}, i::Int64)",
      "   @ Main ./In[109]:46",
      " [2] top-level scope",
      "   @ ./In[129]:13",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# get snp 1\n",
    "snp = 1\n",
    "z = convert(Vector{Float64}, @view(G[:, snp]), center=true)\n",
    "\n",
    "# mathematical Q\n",
    "math_Q = 0.0\n",
    "for i in 1:length(qc_model.data)\n",
    "    gc = qc_model.data[i]\n",
    "    Γ = zeros(gc.n, gc.n)\n",
    "    for k in 1:gc.m # loop over variance components\n",
    "        Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "    end\n",
    "    math_Q += calculate_Qi(qc_model, gc, z, Γ, i)\n",
    "end\n",
    "@show math_Q\n",
    "\n",
    "# autodiff Q\n",
    "autodiff_loglikelihood(β) = QuasiCopula.loglikelihood(β, qc_model, z)\n",
    "∇²logl = x -> ForwardDiff.hessian(autodiff_loglikelihood, x)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0]\n",
    "Hfull = ∇²logl(fullβ)\n",
    "auto_Q = -Hfull[end, end]\n",
    "@show auto_Q;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_resβ_ij = [-0.09693671578820441, -0.07702396925952908, -0.1261277809575639, 0.04040177775644848, 0.04716318537196372]\n",
      "math_resβ_ij = [-0.09693671578820436, -0.04435826891560527, -0.1261277809575638, 0.04040177775644851, 0.04716318537196369]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: fdsa not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: fdsa not defined",
      "",
      "Stacktrace:",
      " [1] calculate_Qi(qc_model::GLMCopulaVCModel{Float64, Bernoulli{Float64}, LogitLink}, gc::GLMCopulaVCObs{Float64, Bernoulli{Float64}, LogitLink}, z::Vector{Float64}, Γ::Matrix{Float64}, i::Int64)",
      "   @ Main ./In[109]:46",
      " [2] top-level scope",
      "   @ In[111]:8",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "for i in 1:10\n",
    "    # mathematical Q for 1 sample\n",
    "    gc = qc_model.data[i]\n",
    "    Γ = zeros(gc.n, gc.n)\n",
    "    for k in 1:gc.m # loop over variance components\n",
    "        Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "    end\n",
    "    Qi_math = calculate_Qi(qc_model, gc, z, Γ, i)\n",
    "\n",
    "    # autodiff Q for 1 sample\n",
    "    autodiff_loglikelihood_i(β) = loglikelihood_i(β, qc_model, z, i)\n",
    "    ∇²logl_i = x -> ForwardDiff.hessian(autodiff_loglikelihood_i, x)\n",
    "    fullβ = [qc_model.β; qc_model.θ; 0.0]\n",
    "    Hfull_i = ∇²logl_i(fullβ)\n",
    "    Qi_auto = -Hfull_i[end, end]\n",
    "    \n",
    "    @show Qi_math, Qi_auto, Qi_math - Qi_auto\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.3382613090493507e-7, -8.106559831189131e-7, -1.3638758370460025e-6], [-6.708739634930794e-7, -1.8532789480829592e-7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc_model.∇β, qc_model.∇θ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0  1.0  1.0  1.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  1.0\n",
       " 1.0  1.0  0.0  1.0  1.0\n",
       " 1.0  1.0  0.0  1.0  1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].y qc_model.data[3].y qc_model.data[7].y  qc_model.data[2].y qc_model.data[4].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Matrix{Float64}:\n",
       " 3.07676  0.960326  11.9462   0.364994  0.218971\n",
       " 2.39065  2.47697    6.80771  2.01004   2.17117"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].q qc_model.data[3].q qc_model.data[7].q    qc_model.data[2].q qc_model.data[4].q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Matrix{Float64}:\n",
       " 2.5  2.5  2.5  2.5  2.5\n",
       " 2.5  2.5  2.5  2.5  2.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].t qc_model.data[3].t qc_model.data[7].t    qc_model.data[2].t qc_model.data[4].t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       "  0.247034   1.93406   0.987779  2.28029     0.0868077\n",
       " -0.212849   0.658095  1.13795   0.696106    0.743739\n",
       "  0.773509   0.90897   0.827584  0.00790315  1.46994\n",
       "  1.50333    0.618312  1.69343   0.265664    0.875086\n",
       "  1.19385   -0.239743  1.44632   1.98587     0.69093"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].η qc_model.data[3].η qc_model.data[7].η    qc_model.data[2].η qc_model.data[4].η]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.561446  0.873698  0.728649  0.907231  0.521688\n",
       " 0.446988  0.658832  0.757303  0.667324  0.677813\n",
       " 0.684279  0.712789  0.695844  0.501976  0.813048\n",
       " 0.818071  0.649835  0.844675  0.566028  0.705803\n",
       " 0.76743   0.44035   0.809431  0.879305  0.666174"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].μ qc_model.data[3].μ qc_model.data[7].μ    qc_model.data[2].μ qc_model.data[4].μ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " -1.13147    0.380211   0.610248   0.319773  -1.04436\n",
       " -0.899043   0.719609  -1.76646   -1.41631   -1.45044\n",
       " -1.47219   -1.57536    0.661138  -1.00396    0.47952\n",
       "  0.47158    0.734066  -2.33197    0.875612   0.645621\n",
       "  0.5505     1.12735   -2.06093    0.370488   0.707891"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].res qc_model.data[3].res qc_model.data[7].res    qc_model.data[2].res qc_model.data[4].res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.246224  0.11035   0.19772   0.0841628  0.24953\n",
       " 0.24719   0.224772  0.183795  0.222003   0.218383\n",
       " 0.216041  0.204721  0.211645  0.249996   0.152001\n",
       " 0.148831  0.22755   0.131199  0.24564    0.207645\n",
       " 0.178481  0.246442  0.154253  0.106127   0.222386"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].dμ qc_model.data[3].dμ qc_model.data[7].dμ    qc_model.data[2].dμ qc_model.data[4].dμ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.246224  0.11035   0.19772   0.0841628  0.24953\n",
       " 0.24719   0.224772  0.183795  0.222003   0.218383\n",
       " 0.216041  0.204721  0.211645  0.249996   0.152001\n",
       " 0.148831  0.22755   0.131199  0.24564    0.207645\n",
       " 0.178481  0.246442  0.154253  0.106127   0.222386"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].varμ qc_model.data[3].varμ qc_model.data[7].varμ    qc_model.data[2].varμ qc_model.data[4].varμ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].w1 qc_model.data[3].w1 qc_model.data[7].w1    qc_model.data[2].w1 qc_model.data[4].w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.246224  0.11035   0.19772   0.0841628  0.24953\n",
       " 0.24719   0.224772  0.183795  0.222003   0.218383\n",
       " 0.216041  0.204721  0.211645  0.249996   0.152001\n",
       " 0.148831  0.22755   0.131199  0.24564    0.207645\n",
       " 0.178481  0.246442  0.154253  0.106127   0.222386"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[qc_model.data[1].w2 qc_model.data[3].w2 qc_model.data[7].w2    qc_model.data[2].w2 qc_model.data[4].w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
