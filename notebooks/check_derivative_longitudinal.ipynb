{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff to check gradient/Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling QuasiCopula [c47b6ae2-b804-4668-9957-eb588c99ffbc]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_make_snparray (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using DataFrames, Random, GLM, QuasiCopula\n",
    "using ForwardDiff, Test, LinearAlgebra\n",
    "using LinearAlgebra: BlasReal, copytri!\n",
    "using ToeplitzMatrices\n",
    "using BenchmarkTools\n",
    "using SnpArrays\n",
    "using ForwardDiff\n",
    "# using MendelPlots\n",
    "ENV[\"COLUMNS\"] = 240\n",
    "\n",
    "\n",
    "BLAS.set_num_threads(1)\n",
    "Threads.nthreads()\n",
    "\n",
    "function simulate_random_snparray(s::Union{String, UndefInitializer}, n::Int64,\n",
    "    p::Int64; mafs::Vector{Float64}=zeros(Float64, p), min_ma::Int = 5)\n",
    "\n",
    "    #first simulate a random {0, 1, 2} matrix with each SNP drawn from Binomial(2, r[i])\n",
    "    A1 = BitArray(undef, n, p) \n",
    "    A2 = BitArray(undef, n, p) \n",
    "    for j in 1:p\n",
    "        minor_alleles = 0\n",
    "        maf = 0\n",
    "        while minor_alleles <= min_ma\n",
    "            maf = 0.5rand()\n",
    "            for i in 1:n\n",
    "                A1[i, j] = rand(Bernoulli(maf))\n",
    "                A2[i, j] = rand(Bernoulli(maf))\n",
    "            end\n",
    "            minor_alleles = sum(view(A1, :, j)) + sum(view(A2, :, j))\n",
    "        end\n",
    "        mafs[j] = maf\n",
    "    end\n",
    "\n",
    "    #fill the SnpArray with the corresponding x_tmp entry\n",
    "    return _make_snparray(s, A1, A2)\n",
    "end\n",
    "\n",
    "function _make_snparray(s::Union{String, UndefInitializer}, A1::BitArray, A2::BitArray)\n",
    "    n, p = size(A1)\n",
    "    x = SnpArray(s, n, p)\n",
    "    for i in 1:(n*p)\n",
    "        c = A1[i] + A2[i]\n",
    "        if c == 0\n",
    "            x[i] = 0x00\n",
    "        elseif c == 1\n",
    "            x[i] = 0x02\n",
    "        elseif c == 2\n",
    "            x[i] = 0x03\n",
    "        else\n",
    "            throw(MissingException(\"matrix shouldn't have missing values!\"))\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qc_model = Quasi-Copula Variance Component Model\n",
      "  * base distribution: Bernoulli\n",
      "  * link function: LogitLink\n",
      "  * number of clusters: 5000\n",
      "  * cluster size min, max: 5, 5\n",
      "  * number of variance components: 2\n",
      "  * number of fixed effects: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function simulate_VC_longitudinal(;\n",
    "    n = 1000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of nongenetic covariates, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = 10, # number of causal SNPs\n",
    "    seed = 2022,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    "    )\n",
    "    m == 1 || m == 2 || error(\"m (number of VC) must be 1 or 2\")\n",
    "    \n",
    "    # non-genetic effect sizes\n",
    "    Random.seed!(seed)\n",
    "#     βtrue = [1.0; rand(-0.03:0.06:0.03, p-1)]\n",
    "    βtrue = [1.0; rand(-5:10:5, p-1)]\n",
    "#     βtrue = [1.0; rand(-2:4:2, p-1)]\n",
    "    dist = y_distribution()\n",
    "    link = canonicallink(dist)\n",
    "    Dist = typeof(dist)\n",
    "    Link = typeof(link)\n",
    "\n",
    "    # variance components\n",
    "    θtrue = fill(0.1, m)\n",
    "    V1 = ones(d, d)\n",
    "    V2 = Matrix(I, d, d)\n",
    "    Γ = m == 1 ? θtrue[1] * V1 : θtrue[1] * V1 + θtrue[2] * V2\n",
    "\n",
    "    # simulate design matrices\n",
    "    Random.seed!(seed)\n",
    "    X_full = [hcat(ones(d), randn(d, p - 1)) for i in 1:n]\n",
    "\n",
    "    # simulate random SnpArray with 100 SNPs and randomly choose k SNPs to be causal\n",
    "    Random.seed!(2022)\n",
    "    G = simulate_random_snparray(undef, n, q)\n",
    "    Gfloat = convert(Matrix{T}, G, center=true, scale=false)\n",
    "    γtrue = zeros(q)\n",
    "    γtrue[1:k] .= rand([-0.2, 0.2], k)\n",
    "    shuffle!(γtrue)\n",
    "    η_G = Gfloat * γtrue\n",
    "\n",
    "    # simulate phenotypes\n",
    "    if y_distribution == Normal\n",
    "        τtrue = 10.0\n",
    "        σ2 = inv(τtrue)\n",
    "        σ = sqrt(σ2)\n",
    "        obs = Vector{GaussianCopulaVCObs{T}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{ContinuousUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i], σ)\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GaussianCopulaVCObs(y, X, V)\n",
    "        end\n",
    "        qc_model = GaussianCopulaVCModel(obs)\n",
    "    else\n",
    "        obs = Vector{GLMCopulaVCObs{T, Dist, Link}}(undef, n)\n",
    "        for i in 1:n\n",
    "            X = X_full[i]\n",
    "            η = X * βtrue\n",
    "            η .+= η_G[i] # add genetic effects\n",
    "            μ = GLM.linkinv.(link, η)\n",
    "            vecd = Vector{DiscreteUnivariateDistribution}(undef, d)\n",
    "            for i in 1:d\n",
    "                vecd[i] = y_distribution(μ[i])\n",
    "            end\n",
    "            nonmixed_multivariate_dist = NonMixedMultivariateDistribution(vecd, Γ)\n",
    "            # simuate single vector y\n",
    "            y = Vector{T}(undef, d)\n",
    "            res = Vector{T}(undef, d)\n",
    "            rand(nonmixed_multivariate_dist, y, res)\n",
    "            V = m == 1 ? [V1] : [V1, V2]\n",
    "            obs[i] = GLMCopulaVCObs(y, X, V, dist, link)\n",
    "        end\n",
    "        qc_model = GLMCopulaVCModel(obs)\n",
    "    end\n",
    "    return qc_model, Γ, G, βtrue, θtrue, γtrue\n",
    "end\n",
    "\n",
    "k = 0 # number of causal SNPs\n",
    "\n",
    "qc_model, Γ, G, βtrue, θtrue, γtrue = simulate_VC_longitudinal(\n",
    "    n = 5000, # sample size\n",
    "    d = 5, # number of observations per sample\n",
    "    p = 3, # number of fixed effects, including intercept\n",
    "    m = 2, # number of variance components\n",
    "    q = 1000, # number of SNPs\n",
    "    k = k, # number of causal SNPs\n",
    "    seed = 1000,\n",
    "    y_distribution = Bernoulli,\n",
    "    T = Float64,\n",
    ")\n",
    "\n",
    "@show qc_model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.13.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        5\n",
      "                     variables with only lower bounds:        2\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  9.4447518e+03 0.00e+00 1.00e+02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "   1  9.1607113e+03 0.00e+00 2.86e+03  -4.3 1.00e+02    -  1.00e+00 3.72e-02f  4\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "   2  9.0179856e+03 0.00e+00 1.99e+03  -2.2 1.79e+03    -  2.93e-04 1.22e-04f 14\n",
      "   3  8.8323880e+03 0.00e+00 1.09e+03  -5.4 2.10e-01    -  1.00e+00 9.58e-01f  1\n",
      "   4  8.7294464e+03 0.00e+00 5.79e+02  -5.1 2.41e-01    -  1.00e+00 1.00e+00f  1\n",
      "   5  8.6575530e+03 0.00e+00 2.33e+02  -5.4 3.91e-01    -  1.00e+00 1.00e+00f  1\n",
      "   6  8.6564182e+03 0.00e+00 1.32e+02  -4.4 3.99e-01    -  1.00e+00 7.34e-01f  1\n",
      "   7  8.6362909e+03 0.00e+00 3.35e+01  -4.6 1.05e-01    -  1.00e+00 1.00e+00f  1\n",
      "Warning: Cutting back alpha due to evaluation error\n",
      "   8  8.6361981e+03 0.00e+00 2.67e+01  -4.5 1.58e+00    -  1.00e+00 1.25e-01f  4\n",
      "   9  8.6365509e+03 0.00e+00 1.44e+01  -3.3 4.76e+00    -  1.00e+00 6.20e-03h  5\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  8.6354037e+03 0.00e+00 3.64e+00  -3.7 1.23e-01    -  1.00e+00 1.00e+00f  1\n",
      "  11  8.6353998e+03 0.00e+00 9.28e-01  -5.4 7.31e-03    -  1.00e+00 2.50e-01f  3\n",
      "  12  8.6353949e+03 0.00e+00 6.70e-01  -7.1 5.87e-03    -  1.00e+00 5.00e-01f  2\n",
      "  13  8.6353928e+03 0.00e+00 1.64e-01  -9.1 7.19e-03    -  1.00e+00 2.50e-01f  3\n",
      "  14  8.6353928e+03 0.00e+00 2.48e-02 -10.3 1.72e-03    -  1.00e+00 5.00e-01f  2\n",
      "  15  8.6353928e+03 0.00e+00 4.05e-02 -11.0 3.28e-03    -  1.00e+00 1.25e-01f  4\n",
      "  16  8.6353928e+03 0.00e+00 1.29e-02 -11.0 3.54e-03    -  1.00e+00 1.25e-01f  4\n",
      "  17  8.6353928e+03 0.00e+00 5.51e-03 -11.0 1.28e-04    -  1.00e+00 1.00e+00f  1\n",
      "  18  8.6353928e+03 0.00e+00 1.63e-02 -11.0 3.51e-04    -  1.00e+00 6.25e-02h  5\n",
      "  19  8.6353928e+03 0.00e+00 1.30e-05 -11.0 1.85e-05    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20  8.6353928e+03 0.00e+00 3.16e-05 -11.0 1.46e-06    -  1.00e+00 1.00e+00f  1\n",
      "  21  8.6353928e+03 0.00e+00 2.18e-04 -11.0 2.43e-06    -  1.00e+00 1.25e-01f  4\n",
      "  22  8.6353928e+03 0.00e+00 1.32e-05 -11.0 1.09e-06    -  1.00e+00 1.00e+00f  1\n",
      "  23  8.6353928e+03 0.00e+00 7.32e-05 -11.0 4.32e-07    -  1.00e+00 1.00e+00f  1\n",
      "  24  8.6353928e+03 0.00e+00 5.80e-05 -11.0 5.79e-07    -  1.00e+00 5.00e-01f  2\n",
      "  25  8.6353928e+03 0.00e+00 6.03e-05 -11.0 2.16e-06    -  1.00e+00 6.25e-02h  5\n",
      "  26  8.6353928e+03 0.00e+00 1.60e-04 -11.0 1.98e-07    -  1.00e+00 5.00e-01f  2\n",
      "  27  8.6353928e+03 0.00e+00 2.42e-05 -11.0 2.41e-07    -  1.00e+00 1.00e+00f  1\n",
      "  28  8.6353928e+03 0.00e+00 2.18e-05 -11.0 1.10e-07    -  1.00e+00 1.25e-01f  4\n",
      "  29  8.6353928e+03 0.00e+00 3.32e-05 -11.0 2.87e-07    -  1.00e+00 2.50e-01f  3\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  30  8.6353928e+03 0.00e+00 1.89e-05 -11.0 1.08e-06    -  1.00e+00 6.25e-02f  5\n",
      "  31  8.6353928e+03 0.00e+00 1.99e-05 -11.0 3.70e-07    -  1.00e+00 5.00e-01f  2\n",
      "  32  8.6353928e+03 0.00e+00 1.87e-05 -11.0 1.18e-07    -  1.00e+00 2.50e-01f  3\n",
      "  33  8.6353928e+03 0.00e+00 1.73e-04 -11.0 2.35e-06    -  1.00e+00 6.25e-02h  5\n",
      "  34  8.6353928e+03 0.00e+00 2.02e-04 -11.0 6.44e-07    -  1.00e+00 5.00e-01f  2\n",
      "  35  8.6353928e+03 0.00e+00 3.51e-05 -11.0 3.67e-07    -  1.00e+00 1.00e+00f  1\n",
      "  36  8.6353928e+03 0.00e+00 6.33e-05 -11.0 4.84e-07    -  1.00e+00 1.25e-01f  4\n",
      "  37  8.6353928e+03 0.00e+00 1.88e-05 -11.0 5.43e-08    -  1.00e+00 1.00e+00f  1\n",
      "  38  8.6353928e+03 0.00e+00 1.17e-05 -11.0 5.21e-08    -  1.00e+00 1.00e+00f  1\n",
      "  39  8.6353928e+03 0.00e+00 9.60e-06 -11.0 2.88e-08    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  40  8.6353928e+03 0.00e+00 2.05e-05 -11.0 3.87e-08    -  1.00e+00 1.00e+00f  1\n",
      "  41  8.6353928e+03 0.00e+00 1.67e-05 -11.0 2.47e-08    -  1.00e+00 1.00e+00f  1\n",
      "  42  8.6353928e+03 0.00e+00 7.63e-05 -11.0 3.58e-06    -  1.00e+00 6.25e-02h  5\n",
      "  43  8.6353928e+03 0.00e+00 5.41e-05 -11.0 8.74e-08    -  1.00e+00 1.00e+00f  1\n",
      "  44  8.6353928e+03 0.00e+00 1.04e-05 -11.0 1.11e-07    -  1.00e+00 1.00e+00f  1\n",
      "  45  8.6353928e+03 0.00e+00 1.20e-05 -11.0 3.60e-07    -  1.00e+00 6.25e-02f  5\n",
      "  46  8.6353928e+03 0.00e+00 3.30e-05 -11.0 2.34e-06    -  1.00e+00 6.25e-02h  5\n",
      "  47  8.6353928e+03 0.00e+00 1.74e-05 -11.0 9.96e-08    -  1.00e+00 1.00e+00f  1\n",
      "  48  8.6353928e+03 0.00e+00 1.31e-04 -11.0 5.32e-06    -  1.00e+00 1.25e-01f  4\n",
      "  49  8.6353928e+03 0.00e+00 7.35e-05 -11.0 2.72e-07    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  50  8.6353928e+03 0.00e+00 2.25e-05 -11.0 3.19e-07    -  1.00e+00 1.00e+00f  1\n",
      "  51  8.6353928e+03 0.00e+00 4.91e-04 -11.0 8.87e-06    -  1.00e+00 6.25e-02h  5\n",
      "  52  8.6353928e+03 0.00e+00 4.14e-04 -11.0 6.57e-07    -  1.00e+00 1.00e+00f  1\n",
      "  53  8.6353928e+03 0.00e+00 9.77e-05 -11.0 1.77e-06    -  1.00e+00 1.00e+00f  1\n",
      "  54  8.6353928e+03 0.00e+00 3.12e-04 -11.0 5.61e-07    -  1.00e+00 5.00e-01f  2\n",
      "  55  8.6353928e+03 0.00e+00 3.70e-05 -11.0 6.95e-07    -  1.00e+00 1.00e+00f  1\n",
      "  56  8.6353928e+03 0.00e+00 1.50e-04 -11.0 3.06e-06    -  1.00e+00 2.50e-01f  3\n",
      "  57  8.6353928e+03 0.00e+00 1.37e-04 -11.0 5.41e-07    -  1.00e+00 6.25e-02h  5\n",
      "  58  8.6353928e+03 0.00e+00 8.49e-05 -11.0 2.56e-06    -  1.00e+00 5.00e-01f  2\n",
      "  59  8.6353928e+03 0.00e+00 1.38e-04 -11.0 1.18e-06    -  1.00e+00 2.50e-01f  3\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  60  8.6353928e+03 0.00e+00 1.64e-05 -11.0 1.88e-06    -  1.00e+00 1.25e-01f  4\n",
      "  61  8.6353928e+03 0.00e+00 3.33e-05 -11.0 1.25e-06    -  1.00e+00 5.00e-01f  2\n",
      "  62  8.6353928e+03 0.00e+00 4.63e-03 -11.0 9.73e-05    -  1.00e+00 6.25e-02h  5\n",
      "  63  8.6353928e+03 0.00e+00 1.76e-04 -11.0 3.54e-06    -  1.00e+00 1.00e+00f  1\n",
      "  64  8.6353928e+03 0.00e+00 1.68e-04 -11.0 1.74e-05    -  1.00e+00 2.50e-01f  3\n",
      "  65  8.6353928e+03 0.00e+00 8.66e-05 -11.0 7.26e-06    -  1.00e+00 6.25e-02f  5\n",
      "  66  8.6353928e+03 0.00e+00 5.57e-05 -11.0 1.25e-06    -  1.00e+00 1.00e+00f  1\n",
      "  67  8.6353928e+03 0.00e+00 4.75e-05 -11.0 1.39e-06    -  1.00e+00 1.25e-01f  4\n",
      "  68  8.6353928e+03 0.00e+00 3.62e-05 -11.0 1.16e-06    -  1.00e+00 5.00e-01f  2\n",
      "  69  8.6353928e+03 0.00e+00 2.40e-05 -11.0 2.57e-07    -  1.00e+00 1.25e-01f  4\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  70  8.6353928e+03 0.00e+00 8.37e-05 -11.0 2.22e-07    -  1.00e+00 5.00e-01f  2\n",
      "  71  8.6353928e+03 0.00e+00 6.77e-05 -11.0 3.30e-06    -  1.00e+00 6.25e-02h  5\n",
      "  72  8.6353928e+03 0.00e+00 1.55e-04 -11.0 7.11e-06    -  1.00e+00 6.25e-02h  5\n",
      "  73  8.6353928e+03 0.00e+00 4.92e-05 -11.0 2.71e-07    -  1.00e+00 1.00e+00f  1\n",
      "  74  8.6353928e+03 0.00e+00 3.84e-05 -11.0 3.39e-06    -  1.00e+00 1.25e-01f  4\n",
      "  75  8.6353928e+03 0.00e+00 4.76e-05 -11.0 2.23e-07    -  1.00e+00 5.00e-01f  2\n",
      "  76  8.6353928e+03 0.00e+00 2.82e-05 -11.0 2.54e-07    -  1.00e+00 2.50e-01f  3\n",
      "  77  8.6353928e+03 0.00e+00 3.11e-04 -11.0 1.13e-05    -  1.00e+00 6.25e-02h  5\n",
      "  78  8.6353928e+03 0.00e+00 1.79e-04 -11.0 5.80e-07    -  1.00e+00 1.00e+00f  1\n",
      "  79  8.6353928e+03 0.00e+00 5.24e-05 -11.0 1.10e-06    -  1.00e+00 5.00e-01f  2\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  80  8.6353928e+03 0.00e+00 9.83e-05 -11.0 9.80e-07    -  1.00e+00 1.25e-01f  4\n",
      "  81  8.6353928e+03 0.00e+00 4.38e-05 -11.0 1.96e-07    -  1.00e+00 1.00e+00f  1\n",
      "  82  8.6353928e+03 0.00e+00 9.08e-05 -11.0 3.35e-07    -  1.00e+00 1.00e+00f  1\n",
      "  83  8.6353928e+03 0.00e+00 5.44e-05 -11.0 2.13e-07    -  1.00e+00 1.00e+00f  1\n",
      "  84  8.6353928e+03 0.00e+00 4.41e-05 -11.0 5.09e-07    -  1.00e+00 2.50e-01f  3\n",
      "  85  8.6353928e+03 0.00e+00 1.71e-04 -11.0 1.30e-07    -  1.00e+00 1.00e+00f  1\n",
      "  86  8.6353928e+03 0.00e+00 2.43e-05 -11.0 1.97e-07    -  1.00e+00 1.00e+00f  1\n",
      "  87  8.6353928e+03 0.00e+00 3.68e-05 -11.0 2.48e-07    -  1.00e+00 1.00e+00f  1\n",
      "  88  8.6353928e+03 0.00e+00 2.36e-05 -11.0 2.19e-07    -  1.00e+00 5.00e-01f  2\n",
      "  89  8.6353928e+03 0.00e+00 1.93e-05 -11.0 1.32e-07    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  90  8.6353928e+03 0.00e+00 2.23e-05 -11.0 1.81e-07    -  1.00e+00 1.00e+00f  1\n",
      "  91  8.6353928e+03 0.00e+00 5.21e-05 -11.0 1.16e-07    -  1.00e+00 1.00e+00f  1\n",
      "  92  8.6353928e+03 0.00e+00 4.31e-05 -11.0 6.13e-08    -  1.00e+00 1.00e+00f  1\n",
      "  93  8.6353928e+03 0.00e+00 1.41e-05 -11.0 2.81e-08    -  1.00e+00 1.00e+00f  1\n",
      "  94  8.6353928e+03 0.00e+00 5.26e-05 -11.0 4.10e-08    -  1.00e+00 1.00e+00f  1\n",
      "  95  8.6353928e+03 0.00e+00 3.82e-05 -11.0 8.05e-08    -  1.00e+00 1.00e+00f  1\n",
      "  96  8.6353928e+03 0.00e+00 1.35e-05 -11.0 7.40e-08    -  1.00e+00 1.00e+00f  1\n",
      "  97  8.6353928e+03 0.00e+00 3.53e-05 -11.0 4.31e-08    -  1.00e+00 1.00e+00f  1\n",
      "  98  8.6353928e+03 0.00e+00 1.31e-05 -11.0 3.03e-08    -  1.00e+00 1.00e+00f  1\n",
      "  99  8.6353928e+03 0.00e+00 1.28e-05 -11.0 5.09e-08    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      " 100  8.6353928e+03 0.00e+00 1.77e-05 -11.0 1.40e-08    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 100\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   2.1053239947271741e+03    8.6353927597931197e+03\n",
      "Dual infeasibility......:   1.7726717859080562e-05    7.2709555127184466e-05\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   9.9999999999999881e-12    4.1016930322461643e-11\n",
      "Overall NLP error.......:   1.7726717859080562e-05    7.2709555127184466e-05\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 441\n",
      "Number of objective gradient evaluations             = 101\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      2.148\n",
      "Total CPU secs in NLP function evaluations           =      2.268\n",
      "\n",
      "EXIT: Maximum Number of Iterations Exceeded.\n",
      " 18.911146 seconds (58.38 M allocations: 2.693 GiB, 2.27% gc time, 86.07% compilation time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Optimization unsuccesful; got UserLimit\n",
      "└ @ QuasiCopula /Users/biona001/.julia/dev/QuasiCopula/src/parameter_estimation/fit_glm_vc.jl:37\n"
     ]
    }
   ],
   "source": [
    "@time optm = QuasiCopula.fit!(qc_model,\n",
    "    Ipopt.IpoptSolver(\n",
    "        print_level = 5, \n",
    "        tol = 10^-6, \n",
    "        max_iter = 100,\n",
    "        accept_after_max_steps = 4,\n",
    "        warm_start_init_point=\"yes\", \n",
    "        limited_memory_max_history = 6, # default value\n",
    "        hessian_approximation = \"limited-memory\",\n",
    "#         derivative_test=\"second-order\"\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "βtrue = [1.0, 5.0, 5.0]\n",
      "qc_model.β = [1.0194775082152694, 5.145091433402387, 5.111460273030911]\n",
      "qc_model.∇β = [3.149173630434543e-5, 7.270955512718447e-5, 6.383652480647373e-5]\n",
      "θtrue = [0.1, 0.1]\n",
      "qc_model.θ = [0.12104905777545721, 0.08450135385070344]\n",
      "qc_model.∇θ = [-5.989331678701859e-5, -6.382187474596179e-5]\n"
     ]
    }
   ],
   "source": [
    "@show βtrue\n",
    "@show qc_model.β\n",
    "@show qc_model.∇β\n",
    "\n",
    "@show θtrue\n",
    "@show qc_model.θ\n",
    "@show qc_model.∇θ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is $\\nabla_\\beta res$ calculated correctly? \n",
    "\n",
    "We can check using ForwardDiff\n",
    "\n",
    "The function is \n",
    "\n",
    "$$res_{ij}(\\beta) = \\frac{y_{ij} - \\mu_{ij}}{\\sqrt{\\sigma_{ij}^2(\\beta)}}$$\n",
    "\n",
    "### Normal\n",
    "\n",
    "Assumes y, X are given. We calculate the residuals for just 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [0.1296459602031359, -1.5158332856428878, 0.19631145529958616, -0.03059313160267374, -0.8880321856615292]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       " -1.0        -1.0\n",
       "  2.07458     2.07458\n",
       " -1.94686    -1.94686\n",
       "  0.0808759   0.0808759\n",
       "  0.154606    0.154606\n",
       " -0.931964   -0.931964\n",
       " -2.26098    -2.26098\n",
       " -1.19819    -1.19819\n",
       "  0.0763038   0.0763038\n",
       " -0.470584   -0.470584\n",
       " -1.03039    -1.03039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(IdentityLink(), η)\n",
    "    varμ = GLM.glmvar.(Normal(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    for i in 1:p, j in 1:d\n",
    "        ∇resβ[j, i] = -X[j, i]\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(qc_model.β) = [0.5344439504497369, 0.02670454905299698, 0.7098215386604426, 0.44787592964791534, 0.08710902598335912]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.267222   -0.267222\n",
       " -0.0133523  -0.0133523\n",
       " -0.354911   -0.354911\n",
       " -0.223938   -0.223938\n",
       " -0.0435545  -0.0435545\n",
       "  0.554373    0.554373\n",
       " -0.025995   -0.025995\n",
       "  0.0287037   0.0287037\n",
       "  0.0346223   0.0346223\n",
       " -0.0405912  -0.0405912\n",
       " -0.604183   -0.604183\n",
       " -0.0159986  -0.0159986\n",
       "  0.0270811   0.0270811\n",
       " -0.105382   -0.105382\n",
       " -0.0448782  -0.0448782"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β::AbstractVector{T}) where T\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - \n",
    "            (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# check objective\n",
    "@show resβ(qc_model.β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resβ(β) = [-0.20116605438121704, 654.5724663515489, -0.04513275044062135, 3.5718353780954617, 70.68722761611653]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       "   -1.73497      -1.73497\n",
       " -329.553      -329.553\n",
       "   -1.00025      -1.00025\n",
       "   -3.03142      -3.03142\n",
       "  -37.1641      -37.1641\n",
       "    3.59933       3.59933\n",
       " -641.594      -641.594\n",
       "    0.0808965     0.0808965\n",
       "    0.468677      0.468677\n",
       "  -34.6356      -34.6356\n",
       "   -3.92273      -3.92273\n",
       " -394.867      -394.867\n",
       "    0.0763233     0.0763233\n",
       "   -1.42654      -1.42654\n",
       "  -38.2936      -38.2936"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β)\n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η) # d by 1\n",
    "    varμ = GLM.glmvar.(Poisson(), μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(LogLink(), η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        ∇resβ[j, i] = x_ji * (-(inv(sqrt(varμ_j)) + (0.5 * inv(varμ_j)) * res_j) * dμ_j)\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# random beta vector\n",
    "β = rand(size(qc_model.data[1].X, 2))\n",
    "\n",
    "# check objective\n",
    "@show resβ(β)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(β)) vec(∇resβ_autodiff(β))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autodiff_loglikelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function A_mul_b!(c::AbstractVector{T}, A::AbstractMatrix, b::AbstractVector) where T\n",
    "    n, p = size(A)\n",
    "    fill!(c, zero(T))\n",
    "    for j in 1:p, i in 1:n\n",
    "        c[i] += A[i, j] * b[j]\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "    ) where T\n",
    "    θ = qc_model.θ\n",
    "    # allocate vector of type T\n",
    "    n, p = size(qc_model.data[1].X)\n",
    "    η = zeros(T, n)\n",
    "    μ = zeros(T, n)\n",
    "    varμ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for (i, gc) in enumerate(qc_model.data)\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(η, X, β)\n",
    "        for i in 1:gc.n\n",
    "            μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "            varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             w2[i] = w1[i] * dμ[i]\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        for j in eachindex(y)\n",
    "            res[j] /= sqrt(varμ[j])\n",
    "        end\n",
    "        # std_res_differential! step (this will compute ∇resβ)\n",
    "#         for i in 1:gc.p\n",
    "#             for j in 1:gc.n\n",
    "#                 ∇resβ[j, i] = -sqrt(varμ[j]) * X[j, i] - (0.5 * res[j] * (1 - (2 * μ[j])) * X[j, i])\n",
    "#             end\n",
    "#         end\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        # component_loglikelihood\n",
    "        for j in 1:gc.n\n",
    "            logl += QuasiCopula.loglik_obs(gc.d, y[j], μ[j], 1.0, 1.0)\n",
    "        end\n",
    "        tsum = dot(θ, gc.t)\n",
    "        logl += -log(1 + tsum)\n",
    "        qsum  = dot(θ, q) # qsum = 0.5 r'Γr\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "function loglikelihood(\n",
    "    β::AbstractVector{T}, \n",
    "    gcm::GaussianCopulaVCModel\n",
    "    ) where T\n",
    "    θ = gcm.θ\n",
    "    τ = gcm.τ[1]\n",
    "    # allocate vector of type T\n",
    "    n, p = size(gcm.data[1].X)\n",
    "    μ = zeros(T, n)\n",
    "    res = zeros(T, n)\n",
    "    storage_n = zeros(T, n)\n",
    "    q = zeros(T, length(θ))\n",
    "    logl = zero(T)\n",
    "    for gc in gcm.data\n",
    "        X = gc.X\n",
    "        y = gc.y\n",
    "        n, p = size(X)\n",
    "        sqrtτ = sqrt(abs(τ))\n",
    "        # update_res! step (need to avoid BLAS)\n",
    "        A_mul_b!(μ, X, β)\n",
    "        for i in 1:gc.n\n",
    "            res[i] = y[i] - μ[i]\n",
    "        end\n",
    "        # standardize_res! step\n",
    "        res .*= sqrtτ\n",
    "        rss  = abs2(norm(res)) # RSS of standardized residual\n",
    "        tsum = dot(θ, gc.t) # ben: why is there abs here?\n",
    "        logl += - log(1 + tsum) - (gc.n * log(2π) -  gc.n * log(abs(τ)) + rss) / 2\n",
    "        # update Γ\n",
    "        @inbounds for k in 1:gc.m\n",
    "            A_mul_b!(storage_n, gc.V[k], res)\n",
    "            q[k] = dot(res, storage_n) / 2 # q[k] = 0.5 r' * V[k] * r (update variable b for variance component model)\n",
    "        end\n",
    "        qsum  = dot(θ, q)\n",
    "        logl += log(1 + qsum)\n",
    "    end\n",
    "    return logl\n",
    "end\n",
    "\n",
    "# sample data\n",
    "autodiff_loglikelihood(β) = loglikelihood(β, qc_model)\n",
    "\n",
    "# function grad_loglikelihood(\n",
    "#     β::AbstractVector{T}, \n",
    "#     qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel}\n",
    "#     ) where T\n",
    "#     β = qc_model.β\n",
    "#     n, p = size(qc_model.data[1].X)\n",
    "#     η = zeros(T, n)\n",
    "#     μ = zeros(T, n)\n",
    "#     varμ = zeros(T, n)\n",
    "#     res = zeros(T, n)\n",
    "#     w1 = zeros(T, n)\n",
    "#     dμ = zeros(T, n)\n",
    "#     storage_n = zeros(T, n)\n",
    "#     out = zeros(length(β))\n",
    "#     for gc in qc_model.data\n",
    "#         X = gc.X\n",
    "#         y = gc.y\n",
    "#         n, p = size(X)\n",
    "#         # update_res! step\n",
    "#         A_mul_b!(η, X, β)\n",
    "#         for i in 1:gc.n\n",
    "#             μ[i] = GLM.linkinv(gc.link, η[i])\n",
    "#             varμ[i] = GLM.glmvar(gc.d, μ[i]) # Note: for negative binomial, d.r is used\n",
    "#             dμ[i] = GLM.mueta(gc.link, η[i])\n",
    "#             w1[i] = dμ[i] / varμ[i]\n",
    "#             res[i] = y[i] - μ[i]\n",
    "#         end\n",
    "#         # GLM gradient\n",
    "#         out += X' * Diagonal(w1) * res\n",
    "#         # 2nd gradient term\n",
    "#         res ./= sqrt.(varμ)\n",
    "        \n",
    "#     end\n",
    "#     return out\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8635.392759793393, -8635.39275979312)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autodiff_loglikelihood(qc_model.β), loglikelihood!(qc_model, true, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta is $\\pm 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 9.659198719758066e-6\n",
       " 2.194933313059977e-5\n",
       " 8.53400730504994e-6"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 3.149173630434543e-5\n",
       " 7.270955512718447e-5\n",
       " 6.383652480647373e-5"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta is $\\pm 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.2863324753430447e-6\n",
       " -7.904796714974793e-7\n",
       "  2.0348114821988617e-6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.286331556300425e-6\n",
       " -7.904736671893176e-7\n",
       "  2.0348091102906363e-6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.2863324753430447e-6\n",
       " -7.904796714974793e-7\n",
       "  2.0348114821988617e-6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " -2.286331556300425e-6\n",
       " -7.904736671893176e-7\n",
       "  2.0348091102906363e-6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my naive gradient (does not work)\n",
    "# function compute_∇resβ(β, X, y, dist, link)\n",
    "#     d, p = size(X)\n",
    "#     ∇resβ = zeros(d, p)\n",
    "#     η = X * β # d by 1\n",
    "#     μ = GLM.linkinv.(link, η) # d by 1\n",
    "#     varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "#     res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "#     for i in 1:p, j in 1:d\n",
    "#         varμ_j = varμ[j]\n",
    "#         x_ji = X[j, i]\n",
    "#         res_j = res[j]\n",
    "#         μ_j = μ[j]\n",
    "#         ∇resβ[j, i] = -sqrt(varμ_j) * x_ji - \n",
    "#             (0.5 * res_j * (1 - 2μ_j) * x_ji)\n",
    "#     end\n",
    "#     return ∇resβ # d × p\n",
    "# end\n",
    "# function grad_logl_sample_i(dist, link, Γ, X, y, β)\n",
    "#     η = X*β\n",
    "#     μ = GLM.linkinv.(link, η)\n",
    "#     varμ = GLM.glmvar.(dist, μ)\n",
    "#     res = (y .- μ) ./ sqrt.(varμ)\n",
    "#     denom = 1 + 0.5 * (res' * Γ * res)\n",
    "#     ∇resβ = compute_∇resβ(β, X, y, dist, link)\n",
    "#     W1 = GLM.mueta.(link, η) / GLM.glmvar.(dist, μ)\n",
    "#     return X' * Diagonal(W1) * (y - μ) + ∇resβ'*Γ*res / denom\n",
    "# end\n",
    "\n",
    "# ∇β_test = zeros(3)\n",
    "# for i in 1:length(qc_model.data)\n",
    "#     Γ = sum(qc_model.θ .* qc_model.data[i].V)\n",
    "#     X = qc_model.data[i].X\n",
    "#     y = qc_model.data[i].y\n",
    "#     ∇β_test += grad_logl_sample_i(Bernoulli(), LogitLink(), Γ, X, y, qc_model.β)\n",
    "# end\n",
    "# ∇β_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  8.57327120407092e-8\n",
       "  2.7007899694453386e-8\n",
       " -1.1578850189764012e-7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "∇βtrue = ∇logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  8.528801986873447e-8\n",
       "  2.6077807413482645e-8\n",
       " -1.1694493039574039e-7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient from math\n",
    "loglikelihood!(qc_model, true, false)\n",
    "∇βobs = qc_model.∇β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla_\\beta^2 L$\n",
    "\n",
    "Hessians for a single observation seems to differ quite a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function two_term_Hessian(gcm::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function two_term_Hessian(gcm::GaussianCopulaVCModel)\n",
    "    p = length(gcm.β)\n",
    "    T = eltype(gcm.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in gcm.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * gc.X\n",
    "        # trailing terms\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = -sqrt(gcm.τ[1]) .* gc.X # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= gcm.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = abs2(1 + 0.5 * (res' * Γ * res))\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function three_term_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "#     # sarah's implementation\n",
    "#     loglikelihood!(qc_model, true, true)\n",
    "#     return qc_model.Hβ\n",
    "#     @show qc_model.Hβ\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)\n",
    "    for gc in qc_model.data\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "# autodiff ∇²resβ (giving some kind of tensor)\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# this is d²rᵢₖ(β) needed for computing the 4th hessian term\n",
    "function r_ik(β, k)\n",
    "    res = resβ(β)\n",
    "    return res[k]\n",
    "end\n",
    "r_ik(β) = r_ik(β, k)\n",
    "∇²r_ik = x -> ForwardDiff.hessian(r_ik, x)\n",
    "\n",
    "function full_hessian(qc_model::Union{GLMCopulaVCModel, NBCopulaVCModel})\n",
    "    p = length(qc_model.β)\n",
    "    T = eltype(qc_model.β)\n",
    "    H = zeros(T, p, p)    \n",
    "    # loop over samples\n",
    "    for (i, gc) in enumerate(qc_model.data)\n",
    "        d = gc.n # number of observations for current sample\n",
    "        # GLM term\n",
    "        H -= Transpose(gc.X) * Diagonal(gc.w2) * gc.X\n",
    "        # 2nd term\n",
    "        res = gc.res # d × 1 standardized residuals\n",
    "        ∇resβ = gc.∇resβ # d × p\n",
    "        Γ = zeros(T, d, d)\n",
    "        for k in 1:gc.m # loop over variance components\n",
    "            Γ .+= qc_model.θ[k] .* gc.V[k]\n",
    "        end\n",
    "        denom = 1 + 0.5 * (res' * Γ * res)\n",
    "        H -= (∇resβ' * Γ * res) * (∇resβ' * Γ * res)' / denom^2\n",
    "        # 3rd term\n",
    "        H += (∇resβ' * Γ * ∇resβ) / denom\n",
    "        # 4th term\n",
    "        ek = zeros(d)    \n",
    "        for k in 1:d\n",
    "            # somehow need to define autodiff functions here, or else k is treated as fixed\n",
    "            function resβ(β)\n",
    "                η = X * β # d by 1\n",
    "                μ = GLM.linkinv.(LogitLink(), η)\n",
    "                varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "                return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "            end\n",
    "            r_ik(β, k) = resβ(β)[k]\n",
    "            r_ik(β) = r_ik(β, k)\n",
    "            ∇²r_ik = x -> ForwardDiff.hessian(r_ik, x) \n",
    "            \n",
    "            \n",
    "            fill!(ek, 0)\n",
    "            ek[k] = 1\n",
    "            X = gc.X\n",
    "            y = gc.y\n",
    "#             @show ∇²r_ik(qc_model.β)\n",
    "            H += (ek' * Γ * res * ∇²r_ik(qc_model.β)) / denom\n",
    "        end\n",
    "    end\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×3 Matrix{Float64}:\n",
       " -0.253406    0.525711    -0.572947\n",
       " -0.279741   -0.544617    -0.335183\n",
       " -0.263081    0.0212769    0.0200741\n",
       " -0.263562    0.0407485   -0.124028\n",
       " -0.272238   -0.253716    -0.280512\n",
       "  0.525711   -1.09063      1.18862\n",
       " -0.544617   -1.06029     -0.652555\n",
       "  0.0212769  -0.00172079  -0.00162351\n",
       "  0.0407485  -0.00629998   0.0191756\n",
       " -0.253716   -0.236454    -0.261427\n",
       " -0.572947    1.18862     -1.29542\n",
       " -0.335183   -0.652555    -0.401613\n",
       "  0.0200741  -0.00162351  -0.00153173\n",
       " -0.124028    0.0191756   -0.0583658\n",
       " -0.280512   -0.261427    -0.289037"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directly evaluating ∇²resβ (giving some kind of tensor)\n",
    "Hββ = ∇²resβ_autodiff(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.253406   0.525711  -0.572947\n",
       "  0.525711  -1.09063    1.18862\n",
       " -0.572947   1.18862   -1.29542"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 1\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -0.279741  -0.544617  -0.335183\n",
       " -0.544617  -1.06029   -0.652555\n",
       " -0.335183  -0.652555  -0.401613"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating hessian of r_ik for k = 1:p\n",
    "k = 2\n",
    "∇²r_ik(qc_model.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -5800.35       28.5649     84.1043\n",
       "    28.5649  -5155.84       59.5103\n",
       "    84.1043     59.5103  -5164.75"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 term Hessian from math\n",
    "two_terms_H = two_term_Hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian from math\n",
    "three_term_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -3684.56       28.4691     61.3107\n",
       "    28.4691  -4268.18       48.266\n",
       "    61.3107     48.266   -4284.97"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 term Hessian implemented by sarah\n",
    "loglikelihood!(qc_model, true, true)\n",
    "qc_model.Hβ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 term Hessian from math\n",
    "full_hessian(qc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " -2793.55       27.4235     44.8972\n",
       "    27.4235  -3385.69       43.6875\n",
       "    44.8972     43.6875  -3393.24"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(\n",
    "        autodiff_loglikelihood, x)\n",
    "autodiff_H = ∇²logl(qc_model.β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check $\\nabla_{\\theta}L$, $\\nabla^2_{\\theta}L$, and $\\nabla_{\\theta}\\nabla_{\\beta} L$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#22 (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loglikelihood function friendly to autodiff\n",
    "autodiff_loglikelihood(β) = QuasiCopula.loglikelihood(β, qc_model, z)\n",
    "\n",
    "# autodiff Gradient\n",
    "∇logl = x -> ForwardDiff.gradient(autodiff_loglikelihood, x)\n",
    "\n",
    "# autodiff Hessian\n",
    "∇²logl = x -> ForwardDiff.hessian(autodiff_loglikelihood, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if `autodiff_loglikelihood` returns same answer as `QuasiCopula.loglikelihood!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff_loglikelihood(fullβ) = -18033.163626812184\n",
      "QuasiCopula.loglikelihood!(qc_model, false, false) = -18033.16362681217\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "# fullβ = [qc_model.β; qc_model.θ; qc_model.τ; 0.0] # normal\n",
    "\n",
    "@show autodiff_loglikelihood(fullβ)\n",
    "@show QuasiCopula.loglikelihood!(qc_model, false, false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.1370986533992892e-6\n",
       "  2.3139215938341664e-6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇logl(fullβ)[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -1.137098653399271e-6\n",
       "  2.3139215938342303e-6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "grad_math = zeros(m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    grad_math += b / (1 + qc_model.θ'*b) - c / (1 + qc_model.θ'*c)\n",
    "end\n",
    "grad_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta^2 L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 1.25985e-15   1.0245e-15\n",
       " 1.0245e-15   -9.38656e-15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[4:5, 4:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " -1.25985e-15  -1.0245e-15\n",
       " -1.0245e-15    9.38656e-15"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "hess_math = zeros(m, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    c = [0.5tr(Ω[k]) for k in 1:m]\n",
    "    hess_math += b*b' / (1 + qc_model.θ'*b)^2 - c*c' / (1 + qc_model.θ'*c)^2\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check $\\nabla_\\theta\\nabla_\\beta L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff (the 4th and 5th position stores the 2 gradient terms with respect to θ)\n",
    "i = 5\n",
    "z = convert(Vector{Float64}, @view(G[:, i]), center=true, scale=false)\n",
    "fullβ = [qc_model.β; qc_model.θ; 0.0] # poisson or bernoulli\n",
    "∇²logl(fullβ)[1:3, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " -1.0073e-7   2.04979e-7\n",
       " -4.43245e-8  9.01974e-8\n",
       " -5.11492e-8  1.04085e-7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical formula\n",
    "Ω = qc_model.data[i].V\n",
    "m = length(Ω)\n",
    "p = size(qc_model.data[i].X, 2)\n",
    "hess_math = zeros(p, m)\n",
    "for i in 1:length(qc_model.data)\n",
    "    r = qc_model.data[i].res\n",
    "    Ω = qc_model.data[i].V\n",
    "    θ = qc_model.θ\n",
    "    ∇resβ = qc_model.data[i].∇resβ\n",
    "    b = [0.5r' * Ω[k] * r for k in 1:m]\n",
    "    A = hcat([∇resβ' * Ω[k] * r for k in 1:m]...)\n",
    "    hess_math += A ./ (1 + θ'*b) - (A*θ ./ (1 + θ'*b)^2) * b'\n",
    "end\n",
    "hess_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check $\\frac{\\partial^2\\mu}{\\partial \\eta^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mueta2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    mueta2(l::Link, η::Real)\n",
    "\n",
    "Second derivative of the inverse link function `d^2μ/dη^2`, for link `L` at linear predictor value `η`.\n",
    "I.e. derivative of the mueta function in GLM.jl\n",
    "\"\"\"\n",
    "function mueta2 end\n",
    "\n",
    "mueta2(::IdentityLink, η::Real) = zero(η)\n",
    "function mueta2(::LogitLink, η::Real)\n",
    "    expabs = exp(-abs(η))\n",
    "    denom = 1 + expabs\n",
    "    return -expabs / denom^2 + 2expabs^2 / denom^3\n",
    "end\n",
    "mueta2(::LogLink, η::Real) = exp(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mueta2(l, η) = -0.015346957645411913\n",
      "mueta2_autodiff(η) = -0.015346957645411843\n"
     ]
    }
   ],
   "source": [
    "# test mueta2 function\n",
    "# l = IdentityLink()\n",
    "# l = LogLink()\n",
    "l = LogitLink()\n",
    "η = 0.1234\n",
    "μ = GLM.linkinv(l, η)\n",
    "\n",
    "# mathematical hessian\n",
    "@show mueta2(l, η)\n",
    "\n",
    "# ForwardDiff Hessian\n",
    "logit_mueta = η -> GLM.mueta(l, η)\n",
    "mueta2_autodiff = x -> ForwardDiff.derivative(logit_mueta, x)\n",
    "@show mueta2_autodiff(η);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial\\sigma^2}{\\partial \\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmamu (generic function with 3 methods)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaeta(D::Distribution, μ::Real)\n",
    "\n",
    "Computes dσ²/dμ\n",
    "\"\"\"\n",
    "function sigmamu end\n",
    "\n",
    "sigmamu(::Normal, μ::Real) = zero(μ)\n",
    "sigmamu(::Bernoulli, μ::Real) = one(μ) - 2μ\n",
    "sigmamu(::Poisson, μ::Real) = one(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\frac{\\partial^2(\\sigma^2)}{\\partial \\mu^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmamu2 (generic function with 3 methods)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sigmaμ2(D::Distribution, μ::Real)\n",
    "\n",
    "Computes d²σ²/dμ²\n",
    "\"\"\"\n",
    "function sigmamu2 end\n",
    "\n",
    "sigmamu2(::Normal, μ::Real) = zero(μ)\n",
    "sigmamu2(::Bernoulli, μ::Real) = -2\n",
    "sigmamu2(::Poisson, μ::Real) = zero(μ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check $\\nabla \\mu$, $\\nabla^2 \\mu$, $\\nabla \\sigma^2$ and $\\nabla^2 \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_μj(X, β, link, j) = 0.8050456535548094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×2 Matrix{Float64}:\n",
       " -0.0957521  -0.0957521\n",
       "  0.198645    0.198645\n",
       " -0.216494   -0.216494\n",
       "  0.198645    0.198645\n",
       " -0.412105   -0.412105\n",
       "  0.449133    0.449133\n",
       " -0.216494   -0.216494\n",
       "  0.449133    0.449133\n",
       " -0.489487   -0.489487"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ∇²μ_j(l::Link, Xi::Matrix, β::Vector, j)\n",
    "\n",
    "Computes the Hessian of the mean function with respect to β for sample i (Xi) at time j\n",
    "\"\"\"\n",
    "function ∇²μ_j(l::Link, Xi::Matrix, β::Vector, j)\n",
    "    xj = Xi[j, :]\n",
    "    ηj = dot(xj, β)\n",
    "    d²μdη² = mueta2(l, ηj)\n",
    "    return d²μdη² * xj * xj'\n",
    "end\n",
    "\n",
    "# objective \n",
    "function eval_μj(X, β, link, j)\n",
    "    η = X*β\n",
    "    μj = GLM.linkinv(link, η[j])\n",
    "    return μj\n",
    "end\n",
    "\n",
    "# autodiff hessian\n",
    "eval_μj(β) = eval_μj(X, β, link, j)\n",
    "∇²μ_j_autodiff = x -> ForwardDiff.hessian(eval_μj, x)\n",
    "\n",
    "# data\n",
    "link = LogitLink()\n",
    "X = qc_model.data[1].X\n",
    "β = qc_model.β\n",
    "j = 1\n",
    "@show eval_μj(X, β, link, j)\n",
    "\n",
    "# compare autodiff and mathematical result\n",
    "math_result = ∇²μ_j(link, X, β, j)\n",
    "autodiff_result = ∇²μ_j_autodiff(β)\n",
    "\n",
    "[vec(math_result) vec(autodiff_result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_σ2j(X, β, dist, link, j) = 0.1569471492473192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9×2 Matrix{Float64}:\n",
       "  0.0091527   0.0091527\n",
       " -0.018988   -0.018988\n",
       "  0.0206941   0.0206941\n",
       " -0.018988   -0.018988\n",
       "  0.0393921   0.0393921\n",
       " -0.0429315  -0.0429315\n",
       "  0.0206941   0.0206941\n",
       " -0.0429315  -0.0429315\n",
       "  0.0467889   0.0467889"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ∇²σ²_j(d::Distribution, l::Link, Xi::Matrix, β::Vector, j)\n",
    "\n",
    "Computes the Hessian of the σ^2 function with respect to β for sample i (Xi) at time j\n",
    "\"\"\"\n",
    "function ∇²σ²_j(d::Distribution, l::Link, Xi::Matrix, β::Vector, j)\n",
    "    xj = Xi[j, :]\n",
    "    ηj = dot(xj, β)\n",
    "    μj = GLM.linkinv.(l, ηj)\n",
    "    c = sigmamu2(d, μj)*GLM.mueta(l, ηj)^2 + sigmamu(d, μj)*mueta2(l, ηj)\n",
    "    return c * xj * xj'\n",
    "end\n",
    "\n",
    "# objective\n",
    "function eval_σ2j(X, β, dist, link, j)\n",
    "    η = X*β\n",
    "    μ = GLM.linkinv.(link, η)\n",
    "    σ2j = GLM.glmvar(dist, μ[j])\n",
    "    return σ2j\n",
    "end\n",
    "\n",
    "# autodiff hessian\n",
    "eval_σ2j(β) = eval_σ2j(X, β, dist, link, j)\n",
    "∇²σ2j_autodiff = x -> ForwardDiff.hessian(eval_σ2j, x)\n",
    "\n",
    "# data\n",
    "dist = Bernoulli()\n",
    "link = LogitLink()\n",
    "X = qc_model.data[1].X\n",
    "β = qc_model.β\n",
    "j = 1\n",
    "@show eval_σ2j(X, β, dist, link, j)\n",
    "\n",
    "# compare autodiff and mathematical result\n",
    "math_result = ∇²σ²_j(dist, link, X, β, j)\n",
    "autodiff_result = ∇²σ2j_autodiff(β)\n",
    "\n",
    "[vec(math_result) vec(autodiff_result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that we can compute $∇resβ$ generally, although we don't do so in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15×2 Matrix{Float64}:\n",
       " -0.307142   -1.1211\n",
       " -0.293194   -1.14587\n",
       " -0.301907   -1.12998\n",
       " -0.30165    -1.13043\n",
       " -0.297072   -1.13862\n",
       "  0.63719     2.3258\n",
       " -0.570809   -2.23085\n",
       "  0.024417    0.0913879\n",
       "  0.046637    0.174771\n",
       " -0.27686    -1.06115\n",
       " -0.694442   -2.53478\n",
       " -0.351303   -1.37297\n",
       "  0.0230367   0.0862215\n",
       " -0.141952   -0.53196\n",
       " -0.3061     -1.17322"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "X = qc_model.data[1].X # d by p\n",
    "y = qc_model.data[1].y # d by 1\n",
    "\n",
    "# objective\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogLink(), η)\n",
    "    varμ = GLM.glmvar.(Poisson(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "\n",
    "# mathematical gradient\n",
    "function ∇resβ(β::AbstractVector{T}) where T\n",
    "    dist = Bernoulli()\n",
    "    link = LogitLink()\n",
    "    \n",
    "    d, p = size(X)\n",
    "    ∇resβ = zeros(T, d, p)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    dμ = GLM.mueta.(link, η) # d by 1\n",
    "    for i in 1:p, j in 1:d\n",
    "        varμ_j = varμ[j]\n",
    "        x_ji = X[j, i]\n",
    "        res_j = res[j]\n",
    "        μ_j = μ[j]\n",
    "        dμ_j = dμ[j]\n",
    "        dμdβ = dμ_j * x_ji\n",
    "        dσ2dβ = sigmamu(dist, μ_j) * dμdβ\n",
    "        # in practice, we have update_∇resβ fucntions to compute ∇resβ[j, i]\n",
    "        ∇resβ[j, i] = -inv(sqrt(varμ_j)) * dμdβ - 0.5 * res_j * inv(varμ_j) * dσ2dβ\n",
    "    end\n",
    "    return ∇resβ # d × p\n",
    "end\n",
    "∇²resβ_autodiff = x -> ForwardDiff.jacobian(∇resβ, x)\n",
    "\n",
    "# autodiff gradient\n",
    "∇resβ_autodiff = x -> ForwardDiff.jacobian(resβ, x)\n",
    "\n",
    "# compare mathematical and numerical gradient\n",
    "[vec(∇resβ(qc_model.β)) vec(∇resβ_autodiff(qc_model.β))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $∇^2resβ$: Hessian of residual vector of sample $i$ at observation $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  0.123026  -0.255227   0.278159\n",
       " -0.255227   0.529487  -0.577062\n",
       "  0.278159  -0.577062   0.628911"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical Hessian of residuals wrt β for sample i at time j\n",
    "# note: need function sigmamu, sigmamu2, mueta2\n",
    "function ∇²resβ_ij(qc_model, i, j)\n",
    "    dist = Bernoulli()\n",
    "    link = LogitLink()\n",
    "    \n",
    "    X = qc_model.data[i].X\n",
    "    y = qc_model.data[i].y\n",
    "    β = qc_model.β\n",
    "    xj = X[j, :]\n",
    "    d, p = size(X)\n",
    "    \n",
    "    # intermediate quantities?\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(link, η) # d by 1\n",
    "    varμ = GLM.glmvar.(dist, μ) # d by 1\n",
    "    res = (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "    invσ = inv.(sqrt.(varμ))\n",
    "    ∇μ_ij  = GLM.mueta(link, η[j]) * xj\n",
    "    ∇σ²_ij = sigmamu(dist, μ[j]) * GLM.mueta(link, η[j]) * xj\n",
    "\n",
    "    # assemble 5 terms\n",
    "    term1 = -invσ[j] * ∇²μ_j(link, X, β, j)\n",
    "    term2 = 0.5invσ[j]^3 * ∇σ²_ij * ∇μ_ij'\n",
    "    term3 = -0.5 * res[j] * inv(varμ[j]) * ∇²σ²_j(dist, link, X, β, j)\n",
    "    term4 = 0.5invσ[j]^3 * ∇μ_ij * ∇σ²_ij'\n",
    "    term5 = 0.75res[j] * inv(varμ[j]^2) * ∇σ²_ij * ∇σ²_ij'\n",
    "    ∇²resβ_ik = term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "    return ∇²resβ_ik # p × p\n",
    "end\n",
    "i = 1 # sample id\n",
    "j = 1 # time point\n",
    "∇²resβ_ij(qc_model, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  0.123026  -0.255227   0.278159\n",
       " -0.255227   0.529487  -0.577062\n",
       "  0.278159  -0.577062   0.628911"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autodiff Hessian of residuals wrt β for sample i at time j\n",
    "β = qc_model.β\n",
    "# gc = qc_model.data[1]\n",
    "T = eltype(β)\n",
    "X = qc_model.data[i].X\n",
    "y = qc_model.data[i].y\n",
    "\n",
    "p = length(β)\n",
    "d = qc_model.data[i].n\n",
    "H = zeros(T, p, p)\n",
    "ek = zeros(T, d)\n",
    "k = 1\n",
    "\n",
    "function resβ(β)\n",
    "    η = X * β # d by 1\n",
    "    μ = GLM.linkinv.(LogitLink(), η)\n",
    "    varμ = GLM.glmvar.(Bernoulli(), μ)\n",
    "    return (y - μ) ./ sqrt.(varμ) # d by 1\n",
    "end\n",
    "r_ik(β, k) = resβ(β)[k]\n",
    "r_ik(β) = r_ik(β, k)\n",
    "∇²r_ik = x -> ForwardDiff.hessian(r_ik, x) \n",
    "∇²r_ik(β)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check 4 Hessian terms one by one\n",
    "\n",
    "When nongenetic covariates have large effects, the Hessian with respect to beta no longer match autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
